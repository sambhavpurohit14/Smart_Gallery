{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:06:46.087265Z",
     "iopub.status.busy": "2025-01-04T06:06:46.086894Z",
     "iopub.status.idle": "2025-01-04T06:06:46.095239Z",
     "shell.execute_reply": "2025-01-04T06:06:46.093823Z",
     "shell.execute_reply.started": "2025-01-04T06:06:46.087232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embed_dim, img_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.img_size = img_size\n",
    "\n",
    "        self.patcher = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=embed_dim,\n",
    "            kernel_size=patch_size,\n",
    "            stride=patch_size\n",
    "        )\n",
    "        self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
    "        self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        x = self.patcher(x)\n",
    "        x = self.flatten(x).permute(0, 2, 1)\n",
    "        x = self.projection(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)\n",
    "        x = x + self.positional_embedding\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:06:47.170235Z",
     "iopub.status.busy": "2025-01-04T06:06:47.169905Z",
     "iopub.status.idle": "2025-01-04T06:06:47.177746Z",
     "shell.execute_reply": "2025-01-04T06:06:47.176432Z",
     "shell.execute_reply.started": "2025-01-04T06:06:47.170207Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "\n",
    "        Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        attention = (Q @ K.transpose(-2, -1)) / self.head_dim**0.5\n",
    "        attention = torch.softmax(attention, dim=-1)\n",
    "        output = (attention @ V).transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        return self.out_projection(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:06:51.291346Z",
     "iopub.status.busy": "2025-01-04T06:06:51.290972Z",
     "iopub.status.idle": "2025-01-04T06:06:51.297321Z",
     "shell.execute_reply": "2025-01-04T06:06:51.296213Z",
     "shell.execute_reply.started": "2025-01-04T06:06:51.291313Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_ratio=4):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(embed_dim, embed_dim * mlp_ratio),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(embed_dim * mlp_ratio, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:06:52.394403Z",
     "iopub.status.busy": "2025-01-04T06:06:52.394029Z",
     "iopub.status.idle": "2025-01-04T06:06:52.401297Z",
     "shell.execute_reply": "2025-01-04T06:06:52.400037Z",
     "shell.execute_reply.started": "2025-01-04T06:06:52.394360Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CNNFeatureExtractor(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        x = self.global_pool(x)\n",
    "        return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:06:53.313903Z",
     "iopub.status.busy": "2025-01-04T06:06:53.313503Z",
     "iopub.status.idle": "2025-01-04T06:06:53.321060Z",
     "shell.execute_reply": "2025-01-04T06:06:53.320018Z",
     "shell.execute_reply.started": "2025-01-04T06:06:53.313872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class HybridCNNViT(nn.Module):\n",
    "    def __init__(self, cnn_channels, vit_embed_dim, num_classes, img_size, patch_size, num_heads, num_layers):\n",
    "        super().__init__()\n",
    "        self.cnn = CNNFeatureExtractor(cnn_channels)\n",
    "        self.vit_embedding = PatchEmbedding(\n",
    "            in_channels=3, patch_size=patch_size, embed_dim=vit_embed_dim, img_size=img_size\n",
    "        )\n",
    "        self.transformer = nn.Sequential(\n",
    "            *[TransformerEncoder(vit_embed_dim, num_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 + vit_embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        cnn_features = self.cnn(x)\n",
    "        vit_embeddings = self.vit_embedding(x)\n",
    "        vit_output = self.transformer(vit_embeddings)\n",
    "        vit_features = vit_output[:, 0]\n",
    "        combined_features = torch.cat([cnn_features, vit_features], dim=1)\n",
    "        return self.fc(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:06:54.307164Z",
     "iopub.status.busy": "2025-01-04T06:06:54.306821Z",
     "iopub.status.idle": "2025-01-04T06:06:55.718057Z",
     "shell.execute_reply": "2025-01-04T06:06:55.717124Z",
     "shell.execute_reply.started": "2025-01-04T06:06:54.307139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "HybridCNNViT (HybridCNNViT)                        [8, 3, 224, 224]     [8, 2]               --                   True\n",
       "├─CNNFeatureExtractor (cnn)                        [8, 3, 224, 224]     [8, 256]             --                   True\n",
       "│    └─Sequential (model)                          [8, 3, 224, 224]     [8, 256, 14, 14]     --                   True\n",
       "│    │    └─Conv2d (0)                             [8, 3, 224, 224]     [8, 32, 224, 224]    896                  True\n",
       "│    │    └─ReLU (1)                               [8, 32, 224, 224]    [8, 32, 224, 224]    --                   --\n",
       "│    │    └─MaxPool2d (2)                          [8, 32, 224, 224]    [8, 32, 112, 112]    --                   --\n",
       "│    │    └─Conv2d (3)                             [8, 32, 112, 112]    [8, 64, 112, 112]    18,496               True\n",
       "│    │    └─ReLU (4)                               [8, 64, 112, 112]    [8, 64, 112, 112]    --                   --\n",
       "│    │    └─MaxPool2d (5)                          [8, 64, 112, 112]    [8, 64, 56, 56]      --                   --\n",
       "│    │    └─Conv2d (6)                             [8, 64, 56, 56]      [8, 128, 56, 56]     73,856               True\n",
       "│    │    └─ReLU (7)                               [8, 128, 56, 56]     [8, 128, 56, 56]     --                   --\n",
       "│    │    └─MaxPool2d (8)                          [8, 128, 56, 56]     [8, 128, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (9)                             [8, 128, 28, 28]     [8, 256, 28, 28]     295,168              True\n",
       "│    │    └─ReLU (10)                              [8, 256, 28, 28]     [8, 256, 28, 28]     --                   --\n",
       "│    │    └─MaxPool2d (11)                         [8, 256, 28, 28]     [8, 256, 14, 14]     --                   --\n",
       "│    └─AdaptiveAvgPool2d (global_pool)             [8, 256, 14, 14]     [8, 256, 1, 1]       --                   --\n",
       "├─PatchEmbedding (vit_embedding)                   [8, 3, 224, 224]     [8, 197, 512]        101,376              True\n",
       "│    └─Conv2d (patcher)                            [8, 3, 224, 224]     [8, 512, 14, 14]     393,728              True\n",
       "│    └─Flatten (flatten)                           [8, 512, 14, 14]     [8, 512, 196]        --                   --\n",
       "│    └─Linear (projection)                         [8, 196, 512]        [8, 196, 512]        262,656              True\n",
       "├─Sequential (transformer)                         [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    └─TransformerEncoder (0)                      [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─MultiHeadAttention (attention)         [8, 197, 512]        [8, 197, 512]        1,050,624            True\n",
       "│    │    └─LayerNorm (norm2)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─Sequential (mlp)                       [8, 197, 512]        [8, 197, 512]        2,099,712            True\n",
       "│    └─TransformerEncoder (1)                      [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─MultiHeadAttention (attention)         [8, 197, 512]        [8, 197, 512]        1,050,624            True\n",
       "│    │    └─LayerNorm (norm2)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─Sequential (mlp)                       [8, 197, 512]        [8, 197, 512]        2,099,712            True\n",
       "│    └─TransformerEncoder (2)                      [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─MultiHeadAttention (attention)         [8, 197, 512]        [8, 197, 512]        1,050,624            True\n",
       "│    │    └─LayerNorm (norm2)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─Sequential (mlp)                       [8, 197, 512]        [8, 197, 512]        2,099,712            True\n",
       "│    └─TransformerEncoder (3)                      [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─MultiHeadAttention (attention)         [8, 197, 512]        [8, 197, 512]        1,050,624            True\n",
       "│    │    └─LayerNorm (norm2)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─Sequential (mlp)                       [8, 197, 512]        [8, 197, 512]        2,099,712            True\n",
       "├─Sequential (fc)                                  [8, 768]             [8, 2]               --                   True\n",
       "│    └─Linear (0)                                  [8, 768]             [8, 512]             393,728              True\n",
       "│    └─ReLU (1)                                    [8, 512]             [8, 512]             --                   --\n",
       "│    └─Dropout (2)                                 [8, 512]             [8, 512]             --                   --\n",
       "│    └─Linear (3)                                  [8, 512]             [8, 2]               1,026                True\n",
       "==================================================================================================================================\n",
       "Total params: 14,150,466\n",
       "Trainable params: 14,150,466\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.64\n",
       "==================================================================================================================================\n",
       "Input size (MB): 4.82\n",
       "Forward/backward pass size (MB): 489.59\n",
       "Params size (MB): 56.20\n",
       "Estimated Total Size (MB): 550.60\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Instantiate the Hybrid Model\n",
    "img_size = 224\n",
    "patch_size = 16\n",
    "vit_embed_dim = 512\n",
    "cnn_channels = 3\n",
    "num_classes = 2\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "\n",
    "model = HybridCNNViT(\n",
    "    cnn_channels=cnn_channels, \n",
    "    vit_embed_dim=vit_embed_dim, \n",
    "    num_classes=num_classes, \n",
    "    img_size=img_size, \n",
    "    patch_size=patch_size, \n",
    "    num_heads=num_heads, \n",
    "    num_layers=num_layers\n",
    ")\n",
    "\n",
    "# Test the Model\n",
    "x = torch.randn(8, 3, 224, 224)\n",
    "output = model(x)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "summary(model=model, \n",
    "        input_size=(8, 3, 224, 224), \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:06:58.314932Z",
     "iopub.status.busy": "2025-01-04T06:06:58.314449Z",
     "iopub.status.idle": "2025-01-04T06:06:58.321041Z",
     "shell.execute_reply": "2025-01-04T06:06:58.320013Z",
     "shell.execute_reply.started": "2025-01-04T06:06:58.314897Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torch.optim import Adam\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# class FocalLoss(nn.Module):\n",
    "#     def __init__(self, gamma=2., alpha=0.25):\n",
    "#         super(FocalLoss, self).__init__()\n",
    "#         self.gamma = gamma\n",
    "#         self.alpha = alpha\n",
    "\n",
    "#     def forward(self, inputs, targets):\n",
    "#         BCE_loss = F.cross_entropy(inputs, targets, reduction='none')\n",
    "#         pt = torch.exp(-BCE_loss)\n",
    "#         F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
    "#         return torch.mean(F_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:06:59.642190Z",
     "iopub.status.busy": "2025-01-04T06:06:59.641833Z",
     "iopub.status.idle": "2025-01-04T06:06:59.649692Z",
     "shell.execute_reply": "2025-01-04T06:06:59.648553Z",
     "shell.execute_reply.started": "2025-01-04T06:06:59.642161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class PatchEmbedding(nn.Module):\n",
    "#     def __init__(self, in_channels, patch_size, embed_dim, img_size):\n",
    "#         super().__init__()\n",
    "#         self.in_channels = in_channels\n",
    "#         self.patch_size = patch_size\n",
    "#         self.embed_dim = embed_dim\n",
    "#         self.img_size = img_size\n",
    "        \n",
    "\n",
    "#         self.patcher = nn.Conv2d(\n",
    "#             in_channels=in_channels,\n",
    "#             out_channels=embed_dim,\n",
    "#             kernel_size=patch_size,\n",
    "#             stride=patch_size\n",
    "#         )\n",
    "#         self.flatten = nn.Flatten(start_dim=2, end_dim=3)\n",
    "#         self.projection = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "#         num_patches = (img_size // patch_size) ** 2\n",
    "#         self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "#         self.positional_embedding = nn.Parameter(torch.randn(1, num_patches + 1, embed_dim))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size = x.shape[0]\n",
    "        \n",
    "#         x = self.patcher(x)\n",
    "#         x = self.flatten(x).permute(0, 2, 1)\n",
    "#         x = self.projection(x)\n",
    "        \n",
    "#         cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "#         x = torch.cat([cls_tokens, x], dim=1)\n",
    "#         x = x + self.positional_embedding\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:07:00.793160Z",
     "iopub.status.busy": "2025-01-04T06:07:00.792818Z",
     "iopub.status.idle": "2025-01-04T06:07:00.800820Z",
     "shell.execute_reply": "2025-01-04T06:07:00.799810Z",
     "shell.execute_reply.started": "2025-01-04T06:07:00.793134Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# class MultiHeadAttention(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads):\n",
    "#         super().__init__()\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = embed_dim // num_heads\n",
    "        \n",
    "#         self.query = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.key = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.value = nn.Linear(embed_dim, embed_dim)\n",
    "#         self.out_projection = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         batch_size, seq_len, embed_dim = x.size()\n",
    "        \n",
    "#         Q = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "#         K = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "#         V = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "#         attention = (Q @ K.transpose(-2, -1)) / self.head_dim**0.5\n",
    "#         attention = torch.softmax(attention, dim=-1)\n",
    "#         output = (attention @ V).transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "#         return self.out_projection(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:07:02.106137Z",
     "iopub.status.busy": "2025-01-04T06:07:02.105736Z",
     "iopub.status.idle": "2025-01-04T06:07:02.112479Z",
     "shell.execute_reply": "2025-01-04T06:07:02.111213Z",
     "shell.execute_reply.started": "2025-01-04T06:07:02.106108Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class TransformerEncoder(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, mlp_ratio=4):\n",
    "#         super().__init__()\n",
    "#         self.norm1 = nn.LayerNorm(embed_dim)\n",
    "#         self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "#         self.norm2 = nn.LayerNorm(embed_dim)\n",
    "#         self.mlp = nn.Sequential(\n",
    "#             nn.Linear(embed_dim, embed_dim * mlp_ratio),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(embed_dim * mlp_ratio, embed_dim)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attention(self.norm1(x))\n",
    "#         x = x + self.mlp(self.norm2(x))\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:07:03.065227Z",
     "iopub.status.busy": "2025-01-04T06:07:03.064894Z",
     "iopub.status.idle": "2025-01-04T06:07:03.071945Z",
     "shell.execute_reply": "2025-01-04T06:07:03.070881Z",
     "shell.execute_reply.started": "2025-01-04T06:07:03.065200Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class CNNFeatureExtractor(nn.Module):\n",
    "#     def __init__(self, in_channels):\n",
    "#         super().__init__()\n",
    "#         self.model = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "#             nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(2, 2)\n",
    "#         )\n",
    "#         self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.model(x)\n",
    "#         x = self.global_pool(x)\n",
    "#         return x.view(x.size(0), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:07:03.898234Z",
     "iopub.status.busy": "2025-01-04T06:07:03.897868Z",
     "iopub.status.idle": "2025-01-04T06:07:03.906049Z",
     "shell.execute_reply": "2025-01-04T06:07:03.905005Z",
     "shell.execute_reply.started": "2025-01-04T06:07:03.898204Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class HybridCNNViT(nn.Module):\n",
    "#     def __init__(self, cnn_channels, vit_embed_dim, num_classes, img_size, patch_size, num_heads, num_layers):\n",
    "#         super().__init__()\n",
    "#         self.cnn = CNNFeatureExtractor(cnn_channels)\n",
    "#         self.vit_embedding = PatchEmbedding(\n",
    "#             in_channels=3, patch_size=patch_size, embed_dim=vit_embed_dim, img_size=img_size\n",
    "#         )\n",
    "#         self.transformer = nn.Sequential(\n",
    "#             *[TransformerEncoder(vit_embed_dim, num_heads) for _ in range(num_layers)]\n",
    "#         )\n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(256 + vit_embed_dim, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(512, num_classes)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         cnn_features = self.cnn(x)\n",
    "#         vit_embeddings = self.vit_embedding(x)\n",
    "#         vit_output = self.transformer(vit_embeddings)\n",
    "#         vit_features = vit_output[:, 0]\n",
    "#         combined_features = torch.cat([cnn_features, vit_features], dim=1)\n",
    "#         return self.fc(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-04T06:07:04.769905Z",
     "iopub.status.busy": "2025-01-04T06:07:04.769473Z",
     "iopub.status.idle": "2025-01-04T06:07:06.207633Z",
     "shell.execute_reply": "2025-01-04T06:07:06.206577Z",
     "shell.execute_reply.started": "2025-01-04T06:07:04.769869Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([8, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==================================================================================================================================\n",
       "Layer (type (var_name))                            Input Shape          Output Shape         Param #              Trainable\n",
       "==================================================================================================================================\n",
       "HybridCNNViT (HybridCNNViT)                        [8, 3, 224, 224]     [8, 2]               --                   True\n",
       "├─CNNFeatureExtractor (cnn)                        [8, 3, 224, 224]     [8, 256]             --                   True\n",
       "│    └─Sequential (model)                          [8, 3, 224, 224]     [8, 256, 14, 14]     --                   True\n",
       "│    │    └─Conv2d (0)                             [8, 3, 224, 224]     [8, 32, 224, 224]    896                  True\n",
       "│    │    └─ReLU (1)                               [8, 32, 224, 224]    [8, 32, 224, 224]    --                   --\n",
       "│    │    └─MaxPool2d (2)                          [8, 32, 224, 224]    [8, 32, 112, 112]    --                   --\n",
       "│    │    └─Conv2d (3)                             [8, 32, 112, 112]    [8, 64, 112, 112]    18,496               True\n",
       "│    │    └─ReLU (4)                               [8, 64, 112, 112]    [8, 64, 112, 112]    --                   --\n",
       "│    │    └─MaxPool2d (5)                          [8, 64, 112, 112]    [8, 64, 56, 56]      --                   --\n",
       "│    │    └─Conv2d (6)                             [8, 64, 56, 56]      [8, 128, 56, 56]     73,856               True\n",
       "│    │    └─ReLU (7)                               [8, 128, 56, 56]     [8, 128, 56, 56]     --                   --\n",
       "│    │    └─MaxPool2d (8)                          [8, 128, 56, 56]     [8, 128, 28, 28]     --                   --\n",
       "│    │    └─Conv2d (9)                             [8, 128, 28, 28]     [8, 256, 28, 28]     295,168              True\n",
       "│    │    └─ReLU (10)                              [8, 256, 28, 28]     [8, 256, 28, 28]     --                   --\n",
       "│    │    └─MaxPool2d (11)                         [8, 256, 28, 28]     [8, 256, 14, 14]     --                   --\n",
       "│    └─AdaptiveAvgPool2d (global_pool)             [8, 256, 14, 14]     [8, 256, 1, 1]       --                   --\n",
       "├─PatchEmbedding (vit_embedding)                   [8, 3, 224, 224]     [8, 197, 512]        101,376              True\n",
       "│    └─Conv2d (patcher)                            [8, 3, 224, 224]     [8, 512, 14, 14]     393,728              True\n",
       "│    └─Flatten (flatten)                           [8, 512, 14, 14]     [8, 512, 196]        --                   --\n",
       "│    └─Linear (projection)                         [8, 196, 512]        [8, 196, 512]        262,656              True\n",
       "├─Sequential (transformer)                         [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    └─TransformerEncoder (0)                      [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─MultiHeadAttention (attention)         [8, 197, 512]        [8, 197, 512]        1,050,624            True\n",
       "│    │    └─LayerNorm (norm2)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─Sequential (mlp)                       [8, 197, 512]        [8, 197, 512]        2,099,712            True\n",
       "│    └─TransformerEncoder (1)                      [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─MultiHeadAttention (attention)         [8, 197, 512]        [8, 197, 512]        1,050,624            True\n",
       "│    │    └─LayerNorm (norm2)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─Sequential (mlp)                       [8, 197, 512]        [8, 197, 512]        2,099,712            True\n",
       "│    └─TransformerEncoder (2)                      [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─MultiHeadAttention (attention)         [8, 197, 512]        [8, 197, 512]        1,050,624            True\n",
       "│    │    └─LayerNorm (norm2)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─Sequential (mlp)                       [8, 197, 512]        [8, 197, 512]        2,099,712            True\n",
       "│    └─TransformerEncoder (3)                      [8, 197, 512]        [8, 197, 512]        --                   True\n",
       "│    │    └─LayerNorm (norm1)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─MultiHeadAttention (attention)         [8, 197, 512]        [8, 197, 512]        1,050,624            True\n",
       "│    │    └─LayerNorm (norm2)                      [8, 197, 512]        [8, 197, 512]        1,024                True\n",
       "│    │    └─Sequential (mlp)                       [8, 197, 512]        [8, 197, 512]        2,099,712            True\n",
       "├─Sequential (fc)                                  [8, 768]             [8, 2]               --                   True\n",
       "│    └─Linear (0)                                  [8, 768]             [8, 512]             393,728              True\n",
       "│    └─ReLU (1)                                    [8, 512]             [8, 512]             --                   --\n",
       "│    └─Dropout (2)                                 [8, 512]             [8, 512]             --                   --\n",
       "│    └─Linear (3)                                  [8, 512]             [8, 2]               1,026                True\n",
       "==================================================================================================================================\n",
       "Total params: 14,150,466\n",
       "Trainable params: 14,150,466\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 6.64\n",
       "==================================================================================================================================\n",
       "Input size (MB): 4.82\n",
       "Forward/backward pass size (MB): 489.59\n",
       "Params size (MB): 56.20\n",
       "Estimated Total Size (MB): 550.60\n",
       "=================================================================================================================================="
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# img_size = 224\n",
    "# patch_size = 16\n",
    "# vit_embed_dim = 512\n",
    "# cnn_channels = 3\n",
    "# num_classes = 2\n",
    "# num_heads = 8\n",
    "# num_layers = 4\n",
    "\n",
    "# hybrid_model = HybridCNNViT(\n",
    "#     cnn_channels=cnn_channels, \n",
    "#     vit_embed_dim=vit_embed_dim, \n",
    "#     num_classes=num_classes, \n",
    "#     img_size=img_size, \n",
    "#     patch_size=patch_size, \n",
    "#     num_heads=num_heads, \n",
    "#     num_layers=num_layers\n",
    "# )\n",
    "\n",
    "# # Define optimizer and scheduler\n",
    "# optimizer = Adam(hybrid_model.parameters(), lr=1e-4)\n",
    "# scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
    "\n",
    "# # For gradient clipping during training\n",
    "# # In your training loop, you would call `clip_grad_norm_`\n",
    "# # Example:\n",
    "# # torch.nn.utils.clip_grad_norm_(hybrid_model.parameters(), max_norm=1.0)\n",
    "\n",
    "# # Test the Model\n",
    "# x = torch.randn(8, 3, 224, 224)\n",
    "# output = hybrid_model(x)\n",
    "# print(f\"Output shape: {output.shape}\")\n",
    "\n",
    "# from torchinfo import summary\n",
    "\n",
    "# summary(model=hybrid_model, \n",
    "#         input_size=(8, 3, 224, 224), \n",
    "#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "#         col_width=20,\n",
    "#         row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
