{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertModel, BertTokenizer\nfrom torchvision import transforms\nfrom PIL import Image\nimport random\nimport json\nfrom torchvision.datasets import CocoCaptions\nfrom torchvision.models import vit_b_16, ViT_B_16_Weights\n\nimage_dir = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\nann_file = '/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json'\n\n\nclass COCOAlignmentDataset(Dataset):\n    def __init__(self, img_dir, annotations_file, transform=None):\n        self.img_dir = img_dir\n        with open(annotations_file, 'r') as f:\n            self.annotations = json.load(f)\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.RandomCrop(224, padding=4),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n        ])\n        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n        \n    def __len__(self):\n        return len(self.annotations['annotations'])\n    \n    def __getitem__(self, idx):\n        ann = self.annotations['annotations'][idx]\n        img_path = f\"{self.img_dir}/{ann['image_id']:012d}.jpg\"\n        image = Image.open(img_path).convert('RGB')\n        image = self.transform(image)\n        \n        # Anchor caption\n        caption = ann['caption']\n        encoding = self.tokenizer(caption, padding='max_length', \n                                truncation=True, max_length=64,\n                                return_tensors='pt')\n        \n        # Get negative caption (random from dataset)\n        neg_idx = random.choice([i for i in range(len(self)) if i != idx])\n        neg_caption = self.annotations['annotations'][neg_idx]['caption']\n        neg_encoding = self.tokenizer(neg_caption, padding='max_length',\n                                    truncation=True, max_length=64,\n                                    return_tensors='pt')\n        \n        return {\n            'image': image,\n            'caption_ids': encoding['input_ids'].squeeze(0),\n            'caption_mask': encoding['attention_mask'].squeeze(0),\n            'neg_caption_ids': neg_encoding['input_ids'].squeeze(0),\n            'neg_caption_mask': neg_encoding['attention_mask'].squeeze(0)\n        }\n\nclass ImageEncoder(nn.Module):\n    def __init__(self, out_dim):\n        super().__init__()\n        weights = ViT_B_16_Weights.IMAGENET1K_V1\n        self.model = vit_b_16(weights=weights)\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n        # Replace the head to output the desired dimension\n        self.model.heads.head = nn.Sequential(\n            nn.Linear(self.model.heads.head.in_features, out_dim),  # Linear layer to `out_dim`\n            nn.LayerNorm(out_dim)  # Layer normalization for better stability\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nclass TextEncoder(nn.Module):\n    def __init__(self, out_dim):\n        super().__init__()\n        self.model = BertModel.from_pretrained('bert-base-uncased')\n        self.projection = nn.Sequential(\n            nn.Linear(768, out_dim),\n            nn.LayerNorm(out_dim)\n        )\n        \n    def forward(self, input_ids, attention_mask):\n        outputs = self.model(input_ids=input_ids, \n                           attention_mask=attention_mask)\n        return self.projection(outputs.pooler_output)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}