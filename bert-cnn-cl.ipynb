{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191},{"sourceId":219323261,"sourceType":"kernelVersion"},{"sourceId":219814374,"sourceType":"kernelVersion"},{"sourceId":219896820,"sourceType":"kernelVersion"}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport random\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\nfrom transformers import AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:36:11.412594Z","iopub.execute_input":"2025-01-30T09:36:11.412905Z","iopub.status.idle":"2025-01-30T09:36:19.780537Z","shell.execute_reply.started":"2025-01-30T09:36:11.412874Z","shell.execute_reply":"2025-01-30T09:36:19.779672Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:36:19.781362Z","iopub.execute_input":"2025-01-30T09:36:19.781718Z","iopub.status.idle":"2025-01-30T09:36:19.785124Z","shell.execute_reply.started":"2025-01-30T09:36:19.781695Z","shell.execute_reply":"2025-01-30T09:36:19.784424Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"class COCOImageCaptionDataset(Dataset):\n    def __init__(self, img_dir, annotations_file, transform=None):\n        self.img_dir = img_dir\n        \n        # Load annotations\n        with open(annotations_file, 'r') as f:\n            self.annotations = json.load(f)\n        \n        # Define default transform if not provided\n        self.transform = transform or transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.RandomGrayscale(p=0.3),\n            transforms.RandomRotation(5),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1),\n            transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Initialize tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\n        # Get list of image files\n        self.image_files = [\n            os.path.join(img_dir, file)\n            for file in os.listdir(img_dir)\n            if file.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))\n        ]\n    \n    def __len__(self):\n        return len(self.annotations['annotations'])\n    \n    def __getitem__(self, idx):\n        # Get annotation\n        ann = self.annotations['annotations'][idx]\n        \n        # Load image\n        img_path = os.path.join(self.img_dir, f\"{ann['image_id']:012d}.jpg\")\n        if not os.path.exists(img_path):\n            img_path = os.path.join(self.img_dir, f\"{ann['image_id']}.jpg\")  # Fallback\n\n        image = Image.open(img_path).convert('RGB')\n        \n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n        \n        # Encode anchor caption\n        caption = ann['caption']\n        encoding = self.tokenizer(caption, padding='max_length', \n                                  truncation=True, max_length=64,\n                                  return_tensors='pt')\n        \n        # Select negative caption safely\n        if len(self.annotations['annotations']) > 1:\n            neg_idx = random.choice([i for i in range(len(self)) if i != idx])\n        else:\n            neg_idx = idx  # Fallback if only one annotation\n        \n        neg_caption = self.annotations['annotations'][neg_idx]['caption']\n        neg_encoding = self.tokenizer(neg_caption, padding='max_length',\n                                      truncation=True, max_length=64,\n                                      return_tensors='pt')\n        \n        return {\n            'image': image,\n            'caption_ids': encoding['input_ids'].squeeze(0),\n            'caption_mask': encoding['attention_mask'].squeeze(0),\n            'neg_caption_ids': neg_encoding['input_ids'].squeeze(0),\n            'neg_caption_mask': neg_encoding['attention_mask'].squeeze(0)\n        }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:36:19.785998Z","iopub.execute_input":"2025-01-30T09:36:19.786313Z","iopub.status.idle":"2025-01-30T09:36:19.813080Z","shell.execute_reply.started":"2025-01-30T09:36:19.786284Z","shell.execute_reply":"2025-01-30T09:36:19.812338Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"img_path = '/kaggle/input/coco-2017-dataset/coco2017/train2017'\ncap_path = '/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json'\n\ntrain_data = COCOImageCaptionDataset(img_path, cap_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:36:19.813766Z","iopub.execute_input":"2025-01-30T09:36:19.813966Z","iopub.status.idle":"2025-01-30T09:36:23.144373Z","shell.execute_reply.started":"2025-01-30T09:36:19.813949Z","shell.execute_reply":"2025-01-30T09:36:23.143714Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30fa81005ae647b2b8b48b83e614e2ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a60a6ad3804438cb849784a6086618d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bb46837b4bb431fb440dc13ec8e2ab5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bad325068a443ce86ecd32c78ebfc10"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"sample = train_data[0]\nsample['image'].shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:36:23.145277Z","iopub.execute_input":"2025-01-30T09:36:23.145595Z","iopub.status.idle":"2025-01-30T09:36:23.434138Z","shell.execute_reply.started":"2025-01-30T09:36:23.145564Z","shell.execute_reply":"2025-01-30T09:36:23.433285Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"torch.Size([3, 224, 224])"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ntrain_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:36:23.436115Z","iopub.execute_input":"2025-01-30T09:36:23.436343Z","iopub.status.idle":"2025-01-30T09:36:23.440098Z","shell.execute_reply.started":"2025-01-30T09:36:23.436323Z","shell.execute_reply":"2025-01-30T09:36:23.439392Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"#utility scripts\nfrom cnn_img_encoder import ImgEncoder_CNN\nfrom bert_encodings import TextEncoder","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:36:23.441030Z","iopub.execute_input":"2025-01-30T09:36:23.441233Z","iopub.status.idle":"2025-01-30T09:36:47.462485Z","shell.execute_reply.started":"2025-01-30T09:36:23.441216Z","shell.execute_reply":"2025-01-30T09:36:47.461474Z"}},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n100%|██████████| 97.8M/97.8M [00:00<00:00, 196MB/s] \n","output_type":"stream"},{"name":"stdout","text":"torch.Size([32, 512])\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b0dec39d5c248a88d9bb22a054f2d07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4ca836ef2974d20aa85540a2a409d9f"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class CLIPModel(nn.Module):\n    def __init__(self, embedding_dim=512):\n        super().__init__()\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.image_encoder = ImgEncoder_CNN(projection_dim=embedding_dim).to(device)\n        self.text_encoder = TextEncoder(projection_dim=embedding_dim).to(device)  \n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n        \n    def forward(self, images, caption_ids, caption_mask, neg_caption_ids, neg_caption_mask):\n        # Get embeddings\n        image_features = self.image_encoder(images)\n        text_features = self.text_encoder(caption_ids, caption_mask)\n        neg_text_features = self.text_encoder(neg_caption_ids, neg_caption_mask)\n        \n        # Normalize features\n        image_features = F.normalize(image_features, p=2, dim=-1)\n        text_features = F.normalize(text_features, p=2, dim=-1)\n        neg_text_features = F.normalize(neg_text_features, p=2, dim=-1)\n        \n        # Scaled pairwise cosine similarities\n        logit_scale = torch.exp(self.logit_scale)\n        pos_logits = (image_features * text_features).sum(dim=-1) * logit_scale\n        neg_logits = (image_features * neg_text_features).sum(dim=-1) * logit_scale\n        \n        return pos_logits, neg_logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:36:47.463370Z","iopub.execute_input":"2025-01-30T09:36:47.463995Z","iopub.status.idle":"2025-01-30T09:36:47.470507Z","shell.execute_reply.started":"2025-01-30T09:36:47.463957Z","shell.execute_reply":"2025-01-30T09:36:47.469576Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import torch.optim as optim \n\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Fixed device\nmodel = CLIPModel().to(device)\nlearning_rate = 3e-4  \noptimizer = optim.AdamW(model.parameters(), lr=learning_rate) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:36:47.471508Z","iopub.execute_input":"2025-01-30T09:36:47.471879Z","iopub.status.idle":"2025-01-30T09:36:48.734831Z","shell.execute_reply.started":"2025-01-30T09:36:47.471842Z","shell.execute_reply":"2025-01-30T09:36:48.733915Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from tqdm import tqdm\n\nnum_epochs = 10 \nloss_history = []\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    \n    # Add progress bar for batches\n    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n\n    for batch in train_loader_tqdm:\n        images = batch['image'].to(device)\n        caption_ids = batch['caption_ids'].to(device)\n        caption_mask = batch['caption_mask'].to(device)\n        neg_caption_ids = batch['neg_caption_ids'].to(device)\n        neg_caption_mask = batch['neg_caption_mask'].to(device)\n\n        \n        pos_logits, neg_logits = model(images, caption_ids, caption_mask, neg_caption_ids, neg_caption_mask)\n\n        loss = -torch.mean(\n            torch.log(torch.sigmoid(pos_logits)) + \n            torch.log(1 - torch.sigmoid(neg_logits))\n        )\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Update progress bar with loss\n        train_loader_tqdm.set_postfix(loss=loss.item())\n\n    avg_loss = total_loss / len(train_loader)\n    loss_history.append(avg_loss)\n    \n    print(f\"Epoch [{epoch+1}/{num_epochs}] - Avg Loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-30T09:37:10.898563Z","iopub.execute_input":"2025-01-30T09:37:10.898898Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/10:   4%|▍         | 704/18493 [11:58<3:30:04,  1.41it/s, loss=0.747]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.plot(range(num_epochs), loss_history, color='red', label='Training Loss')\nplt.title(\"Training Los\") \nplt.xlabel(\"Epochs\")  \nplt.grid(True)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}