{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sambhavpurohit14/Smart_Gallery/blob/fine-tuning/TREC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8cju948cwcO"
      },
      "source": [
        "BERT for classification : with and without fine-tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "r6cGLv31pOG0",
        "outputId": "7f9959de-0d3c-467e-8f61-837133202022"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nget pretrained BERT\\nget TREC dataset\\nadd classification head without fine tune\\nuse Trainer on pretrained BERT base to fine tune it\\ntest before vs after\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "'''\n",
        "get pretrained BERT\n",
        "get TREC dataset\n",
        "add classification head without fine tune\n",
        "use Trainer on pretrained BERT base to fine tune it\n",
        "test before vs after\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "E7ohEVWWFpsj"
      },
      "outputs": [],
      "source": [
        "# pretrained model\n",
        "from transformers import BertModel, BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJqcWXgvhDyk",
        "outputId": "d9ba9688-27b6-4239-9e06-0c37ae34c9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 801
        },
        "id": "VSpjx24GFrL1",
        "outputId": "ba0ead45-34ab-4969-d487-8ebdd8c33f6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 5452\n",
            "Test samples: 500\n",
            "                                                text  coarse_label  fine_label\n",
            "0  How did serfdom develop in and then leave Russ...             2          26\n",
            "1   What films featured the character Popeye Doyle ?             1           5\n",
            "2  How can I find a list of celebrities ' real na...             2          26\n",
            "3  What fowl grabs the spotlight after the Chines...             1           2\n",
            "4                    What is the full form of .com ?             0           1\n",
            "\n",
            "Class distribution (Coarse Labels):\n",
            "coarse_label\n",
            "1    1250\n",
            "3    1223\n",
            "2    1162\n",
            "5     896\n",
            "4     835\n",
            "0      86\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAHYCAYAAABDQl6EAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASsdJREFUeJzt3XlYVPX////HIDIgsogKSCGgWYr7HqmlSeKalpaW5Zr2LszMMrNMTS3K1FzKzPfHXMotLc2syLXMNHdyzT0xFcxIUHoLCOf3h1/m5wgq4uCMnvvtuua6nNd5zTnPM+fM+OA1rzljMQzDEAAAAGASbs4uAAAAALiZCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMDA/xMeHq4ePXo4u4wbNmLECFkslpuyraZNm6pp06a2+z/++KMsFosWLVp0U7bfo0cPhYeH35Rt5WfTpk3y8PDQ0aNHnVbD7apHjx4qWbKkQ9d5+flaEFOnTlX58uWVkZFRoP7OPicL40beM2bOnCmLxaI//vjDsUUBRYwAjNveoUOH9Oyzz6pChQry9PSUr6+vGjVqpIkTJ+p///ufs8u7qtz/XHJvnp6eCgkJUUxMjCZNmqSzZ886ZDsnTpzQiBEjlJCQ4JD1OZIr1/bGG2/oiSeeUFhYWJ5lixcvVqtWrVSmTBl5eHgoJCREjz/+uFavXu2ESm+O8PBwtW3b1tllOFSPHj2UmZmpTz755KZvOzw83O71f6XbzJkzb3ptrmLdunVq1aqV7rjjDnl6eqp8+fJq166d5s6dW6j1TZkyxdTPp5lYDMMwnF0EUFS+/fZbPfbYY7JarerWrZuqVaumzMxMrVu3Tl9++aV69OihadOmSbr4n03Tpk1d6s1v5syZ6tmzp0aOHKmIiAhlZWUpKSlJP/74o1asWKHy5ctr6dKlqlGjhu0xFy5c0IULF+Tp6Vng7WzZskX169fXjBkzrmsUPDMzU5Lk4eEh6eIIcLNmzbRw4UJ16tSpwOspbG1ZWVnKycmR1Wp1yLauR0JCgmrXrq3169crKirK1m4Yhnr16qWZM2eqdu3a6tSpk4KDg3Xy5EktXrxYW7du1S+//KL77rvvptdc1MLDw1WtWjUtW7bshtfVo0cPLVq0SOfOnXNAZRfljv7++OOP1/W4wYMHa8GCBTpy5Mg1R0odeU4uWbLEbv+/++47zZs3Tx988IHKlClja7/vvvtUoUKFQm+nMO8ZubKzs5WVlSWr1XrTPnnKtXDhQnXu3Fm1atVSly5dVKpUKR05ckRr165V8eLFtWbNmuteZ7Vq1VSmTJnrPkdw63F3dgFAUTly5Ii6dOmisLAwrV69WuXKlbMti42N1cGDB/Xtt986scKCa9WqlerVq2e7P2TIEK1evVpt27bVww8/rL1798rLy0uS5O7uLnf3on1p//vvvypRooQt+DpL8eLFnbbtGTNmqHz58rr33nvt2seNG6eZM2dqwIABGj9+vF0oeOONN/TZZ58V+fG51IULF5STk+P0Y3Ure/zxxzVmzBitWbNGDz744FX7OvKc7NChg939pKQkzZs3Tx06dLjqNIv09HR5e3sXeDs38p5RrFgxFStWrFCPvVEjRoxQZGSkfv311zzn96lTp5xSE24dTIHAbWvMmDE6d+6cpk+fbhd+c91111168cUXr/j4lJQUvfLKK6pevbpKliwpX19ftWrVSr/99luevpMnT1bVqlVVokQJlSpVSvXq1bP7CO7s2bMaMGCAwsPDZbVaFRgYqIceekjbtm0r9P49+OCDevPNN3X06FF9/vnntvb85vOtWLFCjRs3lr+/v0qWLKl77rlHr7/+uqSLo2H169eXJPXs2TPPx6pNmzZVtWrVtHXrVt1///0qUaKE7bFXmlOZnZ2t119/XcHBwfL29tbDDz+sY8eO2fW50pzrS9d5rdrym2+Znp6ul19+WaGhobJarbrnnns0duxYXf5hl8ViUb9+/bRkyRJVq1ZNVqtVVatWVXx8fP5P+GWWLFmiBx980O65/t///qe4uDhVrlxZY8eOzXdE7Omnn1aDBg1s9w8fPqzHHntMAQEBKlGihO699948f5hlZmZq2LBhqlu3rvz8/OTt7a0mTZrkGeH6448/ZLFYNHbsWE2YMEEVK1aU1WrVnj17JF37PJWk48ePq1evXgoKCrI9J59++mmBnpOC+Pnnn/XYY4+pfPnyslqtCg0N1UsvvXTF6UiHDx9WTEyMvL29FRISopEjR+Y5ljk5OZowYYKqVq0qT09PBQUF6dlnn9U///xzzXoK8pzUrVtXAQEB+vrrr6+5vsvPyUuPybRp02zHpH79+tq8efM111eQ7ZUsWVKHDh1S69at5ePjo65du0oq+HOd33tGQV8f+c0Bzp0Ks27dOjVo0ECenp6qUKGCZs+enaf+HTt26IEHHpCXl5fuvPNOjR49WjNmzCjQvOJDhw6pfv36+f5xFxgYaHe/IOdIeHi4du/erZ9++sn2XnO9c8Zx62AEGLetb775RhUqVCj0R82HDx/WkiVL9NhjjykiIkLJycn65JNP9MADD2jPnj0KCQmRJP33v/9V//791alTJ7344os6f/68duzYoY0bN+rJJ5+UJP3nP//RokWL1K9fP0VGRurvv//WunXrtHfvXtWpU6fQ+/j000/r9ddf1/Lly9WnT598++zevVtt27ZVjRo1NHLkSFmtVh08eFC//PKLJKlKlSoaOXKkhg0bpr59+6pJkyaSZPe8/f3332rVqpW6dOmip556SkFBQVet6+2335bFYtHgwYN16tQpTZgwQdHR0UpISLCNVBdEQWq7lGEYevjhh7VmzRr17t1btWrV0g8//KBBgwbp+PHj+uCDD+z6r1u3Tl999ZWef/55+fj4aNKkSerYsaMSExNVunTpK9Z1/PhxJSYm5jl269atU0pKigYMGFCgUbHk5GTdd999+vfff9W/f3+VLl1as2bN0sMPP6xFixbpkUcekSSlpaXp//7v//TEE0+oT58+Onv2rKZPn66YmBht2rRJtWrVslvvjBkzdP78efXt21dWq1UBAQEFOk+Tk5N177332sJP2bJl9f3336t3795KS0vTgAEDrrlP17Jw4UL9+++/eu6551S6dGlt2rRJkydP1p9//qmFCxfa9c3OzlbLli117733asyYMYqPj9fw4cN14cIFjRw50tbv2WeftU0X6t+/v44cOaIPP/xQ27dv1y+//HLFUdmCPCe56tSpY3vNFMbcuXN19uxZPfvss7JYLBozZoweffRRHT58+IZHjS9cuKCYmBg1btxYY8eOVYkSJSRd33Odn8K+PiTp4MGD6tSpk3r37q3u3bvr008/VY8ePVS3bl1VrVpV0sXXUbNmzWSxWDRkyBB5e3vr//7v/wo8fSQsLEyrVq3Sn3/+qTvvvPOqfQtyjkyYMEEvvPCCSpYsqTfeeEOSrvleh1uYAdyGUlNTDUlG+/btC/yYsLAwo3v37rb758+fN7Kzs+36HDlyxLBarcbIkSNtbe3btzeqVq161XX7+fkZsbGxBa4l14wZMwxJxubNm6+67tq1a9vuDx8+3Lj0pf3BBx8Ykoy//vrriuvYvHmzIcmYMWNGnmUPPPCAIcmYOnVqvsseeOAB2/01a9YYkow77rjDSEtLs7V/8cUXhiRj4sSJtrbLn+8rrfNqtXXv3t0ICwuz3V+yZIkhyRg9erRdv06dOhkWi8U4ePCgrU2S4eHhYdf222+/GZKMyZMn59nWpVauXGlIMr755hu79okTJxqSjMWLF1/18bkGDBhgSDJ+/vlnW9vZs2eNiIgIIzw83Hb+XbhwwcjIyLB77D///GMEBQUZvXr1srUdOXLEkGT4+voap06dsutfkPO0d+/eRrly5YzTp0/btXfp0sXw8/Mz/v3336s+PiwszGjTps1V++S3jri4OMNisRhHjx61tXXv3t2QZLzwwgu2tpycHKNNmzaGh4eH7Xz++eefDUnGnDlz7NYZHx+fp/3yc6sgz0muvn37Gl5eXtfsd/k5mXtMSpcubaSkpNjav/7663zPoat5//33DUnGkSNH7LYnyXjttdfy9C/oc335e4ZhFPz1kfsedWlNYWFhhiRj7dq1trZTp04ZVqvVePnll21tL7zwgmGxWIzt27fb2v7++28jICAgzzrzM336dFudzZo1M958803j559/zvO+fT3nSNWqVe3OEdy+mAKB21JaWpokycfHp9DrsFqtcnO7+BLJzs7W33//bZs+cOnUBX9/f/35559X/TjT399fGzdu1IkTJwpdz5WULFnyqleD8Pf3lyR9/fXXysnJKdQ2rFarevbsWeD+3bp1s3vuO3XqpHLlyum7774r1PYL6rvvvlOxYsXUv39/u/aXX35ZhmHo+++/t2uPjo5WxYoVbfdr1KghX19fHT58+Krb+fvvvyVJpUqVsmu/3vPuu+++U4MGDdS4cWNbW8mSJdW3b1/98ccftqkLxYoVs33Mm5OTo5SUFF24cEH16tXLdxpNx44dVbZsWbu2a52nhmHoyy+/VLt27WQYhk6fPm27xcTEKDU19Yam7OS69BOA9PR0nT59Wvfdd58Mw9D27dvz9O/Xr5/t37kj05mZmVq5cqWki6Ocfn5+euihh+xqrlu3rkqWLHnVL0IV5LWbq1SpUvrf//6nf//993p216Zz585250vupxnXOtcK6rnnnsvTdr3P9eUK+/qQpMjISNs+SlLZsmV1zz332D02Pj5eUVFRdp9gBAQE2KZwXEuvXr0UHx+vpk2bat26dRo1apSaNGmiSpUqaf369bZ+N3KO4PZFAMZtydfXV5Ju6DJhOTk5+uCDD1SpUiVZrVaVKVNGZcuW1Y4dO5SammrrN3jwYJUsWVINGjRQpUqVFBsbm+ej0jFjxmjXrl0KDQ1VgwYNNGLECIf9x3fu3LmrBq7OnTurUaNGeuaZZxQUFKQuXbroiy++uK4wfMcdd1zXl6gqVapkd99iseiuu+4q8muFHj16VCEhIXmejypVqtiWX6p8+fJ51lGqVKkCzR2VlGcu6vWed0ePHtU999yTpz2/emfNmqUaNWrI09NTpUuXVtmyZfXtt9/anYu5IiIi8rRd6zz966+/dObMGU2bNk1ly5a1u+X+8eOILxYlJiaqR48eCggIUMmSJVW2bFk98MADkpRnX9zc3PJc3eDuu++WJNu5dODAAaWmpiowMDBP3efOnbtqzQV57ebKPdaFvdLB5edabhgu6Ll2Ne7u7vlOAbie57ogNUsFf30U5LFHjx7VXXfdladffm1XEhMTox9++EFnzpzR2rVrFRsbq6NHj6pt27a2Y38j5whuX8wBxm3J19dXISEh2rVrV6HX8c477+jNN99Ur169NGrUKAUEBMjNzU0DBgywC49VqlTRvn37tGzZMsXHx+vLL7/UlClTNGzYML311luSLn6LvEmTJlq8eLGWL1+u999/X++9956++uortWrVqtA1/vnnn0pNTb3qfxheXl5au3at1qxZo2+//Vbx8fFasGCBHnzwQS1fvrxAc1WvZ95uQV0pSGRnZ9+0b5VfaTuXB9vL5c5/vDwIVK5cWZK0c+fOPN/gvxGff/65evTooQ4dOmjQoEEKDAxUsWLFFBcXp0OHDuXpn9/xutZ5mntOP/XUU+revXu+dVx6ub3CyM7O1kMPPaSUlBQNHjxYlStXlre3t44fP64ePXoU6hOKnJwcBQYGas6cOfkuv3wk/FIFee3m+ueff1SiRIlCvxYKe64VxKWfVuVyxHN9IzUX5f7mp0SJEmrSpImaNGmiMmXK6K233tL333+v7t2739A5gtsXARi3rbZt22ratGnasGGD3XVaC2rRokVq1qyZpk+fbtd+5swZu2twSpK3t7c6d+6szp07KzMzU48++qjefvttDRkyxHZtzXLlyun555/X888/r1OnTqlOnTp6++23bygAf/bZZ5IujoJcjZubm5o3b67mzZtr/Pjxeuedd/TGG29ozZo1io6Odvj1Ow8cOGB33zAMHTx40C5AlSpVSmfOnMnz2KNHj9qN+l1PbWFhYVq5cqXOnj1rNwr8+++/25Y7Qm7QPXLkiF1748aNVapUKc2bN0+vv/76NYN8WFiY9u3bl6f98noXLVqkChUq6KuvvrJ7PoYPH35ddV/tPC1btqx8fHyUnZ2t6Ojo61pvQe3cuVP79+/XrFmz1K1bN1v7ihUr8u2fk5Ojw4cP20Z9JWn//v2SZLvSQsWKFbVy5Uo1atSoUOG0IK9d6eKxzh2ZvxVc73PtDGFhYTp48GCe9vzarkfuJSNPnjwp6frOkZt9LWM4D1MgcNt69dVX5e3trWeeeUbJycl5lh86dEgTJ0684uOLFSuWZ7Ri4cKFOn78uF1b7nzQXB4eHoqMjJRhGMrKylJ2dnaejxsDAwMVEhJS4J9Xzc/q1as1atQoRUREXHXOXEpKSp623Dl3udvPvWZofoG0MGbPnm03DWDRokU6efKkXdivWLGifv31V9uPaUjSsmXL8lwu7Xpqa926tbKzs/Xhhx/atX/wwQeyWCw39MfGpe644w6FhoZqy5Ytdu0lSpTQ4MGDtXfvXg0ePDjf0a7PP/9cmzZtstW7adMmbdiwwbY8PT1d06ZNU3h4uCIjIyX9/6Npl65v48aNdo+7lmudp8WKFVPHjh315Zdf5vvJyV9//VXgbV1JfvthGMZVX4eXHkvDMPThhx+qePHiat68uaSLn65kZ2dr1KhReR574cKFq54313pOLrVt27Zb6sdLCvNc32wxMTHasGGD3a88pqSkXHGk9nKrVq3Ktz33uwa504uu5xzx9vZ22PsgXBsjwLhtVaxYUXPnzlXnzp1VpUoVu1+CW79+vRYuXHjVXz1r27atRo4cqZ49e+q+++7Tzp07NWfOnDxzElu0aKHg4GA1atRIQUFB2rt3rz788EO1adNGPj4+OnPmjO6880516tRJNWvWVMmSJbVy5Upt3rxZ48aNK9C+fP/99/r999914cIFJScna/Xq1VqxYoXCwsK0dOnSq/6C08iRI7V27Vq1adNGYWFhOnXqlKZMmaI777zT9uWrihUryt/fX1OnTpWPj4+8vb3VsGHDfOeSFkRAQIAaN26snj17Kjk5WRMmTNBdd91ld6m2Z555RosWLVLLli31+OOP69ChQ/r888/tvnRzvbW1a9dOzZo10xtvvKE//vhDNWvW1PLly/X1119rwIABedZ9I9q3b6/FixfLMAy7UaNBgwZp9+7dGjdunNasWWP7JbikpCQtWbJEmzZtsn1B57XXXtO8efPUqlUr9e/fXwEBAZo1a5aOHDmiL7/80vaxdtu2bfXVV1/pkUceUZs2bXTkyBFNnTpVkZGRBf6ltGudp5L07rvvas2aNWrYsKH69OmjyMhIpaSkaNu2bVq5cmW+f0xd7uDBgxo9enSe9tq1a6tFixaqWLGiXnnlFR0/fly+vr768ssvrzin1NPTU/Hx8erevbsaNmyo77//Xt9++61ef/1128fWDzzwgJ599lnFxcUpISFBLVq0UPHixXXgwAEtXLhQEydOvOKvEhbkOZGkrVu3KiUlRe3bt7/m/ruKypUrX9dz7QyvvvqqPv/8cz300EN64YUXbJdBK1++vFJSUq45Gtu+fXtFRESoXbt2qlixotLT07Vy5Up98803ql+/vtq1ayfp+s6RunXr6uOPP9bo0aN11113KTAw8Jo/foJb1M274ATgHPv37zf69OljhIeHGx4eHoaPj4/RqFEjY/Lkycb58+dt/fK7DNrLL79slCtXzvDy8jIaNWpkbNiwIc+llD755BPj/vvvN0qXLm1YrVajYsWKxqBBg4zU1FTDMAwjIyPDGDRokFGzZk3Dx8fH8Pb2NmrWrGlMmTLlmrXnXmIo9+bh4WEEBwcbDz30kDFx4kS7S43luvySRqtWrTLat29vhISEGB4eHkZISIjxxBNPGPv377d73Ndff21ERkYa7u7udpcde+CBB654qagrXQZt3rx5xpAhQ4zAwEDDy8vLaNOmjd1ll3KNGzfOuOOOOwyr1Wo0atTI2LJlS551Xq22yy85ZRgXLyP20ksvGSEhIUbx4sWNSpUqGe+//76Rk5Nj109Svpemu9Ll2S63bdu2PJcwu9SiRYuMFi1aGAEBAYa7u7tRrlw5o3PnzsaPP/5o1+/QoUNGp06dDH9/f8PT09No0KCBsWzZMrs+OTk5xjvvvGOEhYUZVqvVqF27trFs2bIrXnLr/fffz1PPtc7TXMnJyUZsbKwRGhpqFC9e3AgODjaaN29uTJs27ZrPSe7lr/K79e7d2zAMw9izZ48RHR1tlCxZ0ihTpozRp08f2+W1Lr3UXffu3Q1vb2/j0KFDRosWLYwSJUoYQUFBxvDhw/Nc5sowDGPatGlG3bp1DS8vL8PHx8eoXr268eqrrxonTpyw9bne126uwYMHG+XLl89zDuXneo6JJGP48OHXXGeuK10GzdvbO9/+BX2ur3QZtIK8Pq50GbT8LoeX32t7+/btRpMmTQyr1WrceeedRlxcnDFp0iRDkpGUlHTlJ8MwjHnz5hldunQxKlasaHh5eRmenp5GZGSk8cYbb+T73liQcyQpKclo06aN4ePjY0jikmi3MYthFNGMdAC4zTVv3lwhISG2udi4/WRkZCg8PFyvvfbaVX85Eo4zYMAAffLJJzp37pzTfmYZtz/mAANAIb3zzjtasGBBnsur4fYxY8YMFS9eXP/5z3+cXcpt6fKfZf7777/12WefqXHjxoRfFClGgAEAgFPUqlVLTZs2VZUqVZScnKzp06frxIkTWrVqle6//35nl4fbGF+CAwAATtG6dWstWrRI06ZNk8ViUZ06dTR9+nTCL4ocI8AAAAAwFeYAAwAAwFQIwAAAADAV5gAXQE5Ojk6cOCEfHx9+JhEAAMAFGYahs2fPKiQkxPZDQldCAC6AEydOKDQ01NllAAAA4BqOHTumO++886p9CMAFkPuTmMeOHZOvr6+TqwEAAMDl0tLSFBoaavdT5ldCAC6A3GkPvr6+BGAAAAAXVpDpqnwJDgAAAKZCAAYAAICpEIABAABgKgRgAAAAmAoBGAAAAKZCAAYAAICpEIABAABgKgRgAAAAmAoBGAAAAKZCAAYAAICpEIABAABgKgRgAAAAmAoBGAAAAKZCAAYAAICpEIABAABgKu7OLgD23t1+2tklONRrtcs4uwQAAAA7jAADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyF6wAD1+F2uk4z12gGAJgVI8AAAAAwFQIwAAAATMWpAXjt2rVq166dQkJCZLFYtGTJEtuyrKwsDR48WNWrV5e3t7dCQkLUrVs3nThxwm4dKSkp6tq1q3x9feXv76/evXvr3Llzdn127NihJk2ayNPTU6GhoRozZszN2D0AAAC4IKcG4PT0dNWsWVMfffRRnmX//vuvtm3bpjfffFPbtm3TV199pX379unhhx+269e1a1ft3r1bK1as0LJly7R27Vr17dvXtjwtLU0tWrRQWFiYtm7dqvfff18jRozQtGnTinz/AAAA4Hqc+iW4Vq1aqVWrVvku8/Pz04oVK+zaPvzwQzVo0ECJiYkqX7689u7dq/j4eG3evFn16tWTJE2ePFmtW7fW2LFjFRISojlz5igzM1OffvqpPDw8VLVqVSUkJGj8+PF2QRkAAADmcEvNAU5NTZXFYpG/v78kacOGDfL397eFX0mKjo6Wm5ubNm7caOtz//33y8PDw9YnJiZG+/bt0z///JPvdjIyMpSWlmZ3AwAAwO3hlgnA58+f1+DBg/XEE0/I19dXkpSUlKTAwEC7fu7u7goICFBSUpKtT1BQkF2f3Pu5fS4XFxcnPz8/2y00NNTRuwMAAAAnuSUCcFZWlh5//HEZhqGPP/64yLc3ZMgQpaam2m7Hjh0r8m0CAADg5nD5H8LIDb9Hjx7V6tWrbaO/khQcHKxTp07Z9b9w4YJSUlIUHBxs65OcnGzXJ/d+bp/LWa1WWa1WR+4GAAAAXIRLjwDnht8DBw5o5cqVKl26tN3yqKgonTlzRlu3brW1rV69Wjk5OWrYsKGtz9q1a5WVlWXrs2LFCt1zzz0qVarUzdkRAAAAuAynBuBz584pISFBCQkJkqQjR44oISFBiYmJysrKUqdOnbRlyxbNmTNH2dnZSkpKUlJSkjIzMyVJVapUUcuWLdWnTx9t2rRJv/zyi/r166cuXbooJCREkvTkk0/Kw8NDvXv31u7du7VgwQJNnDhRAwcOdNZuAwAAwImcOgViy5Ytatasme1+bijt3r27RowYoaVLl0qSatWqZfe4NWvWqGnTppKkOXPmqF+/fmrevLnc3NzUsWNHTZo0ydbXz89Py5cvV2xsrOrWrasyZcpo2LBhXAINAADApJwagJs2bSrDMK64/GrLcgUEBGju3LlX7VOjRg39/PPP110fAAAAbj8uPQcYAAAAcDQCMAAAAEyFAAwAAABTIQADAADAVFz+hzAAoCDe3X7a2SU41Gu1yzi7BAC4bTECDAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFacG4LVr16pdu3YKCQmRxWLRkiVL7JYbhqFhw4apXLly8vLyUnR0tA4cOGDXJyUlRV27dpWvr6/8/f3Vu3dvnTt3zq7Pjh071KRJE3l6eio0NFRjxowp6l0DAACAi3JqAE5PT1fNmjX10Ucf5bt8zJgxmjRpkqZOnaqNGzfK29tbMTExOn/+vK1P165dtXv3bq1YsULLli3T2rVr1bdvX9vytLQ0tWjRQmFhYdq6davef/99jRgxQtOmTSvy/QMAAIDrcXfmxlu1aqVWrVrlu8wwDE2YMEFDhw5V+/btJUmzZ89WUFCQlixZoi5dumjv3r2Kj4/X5s2bVa9ePUnS5MmT1bp1a40dO1YhISGaM2eOMjMz9emnn8rDw0NVq1ZVQkKCxo8fbxeUL5WRkaGMjAzb/bS0NAfvOQAAAJzFZecAHzlyRElJSYqOjra1+fn5qWHDhtqwYYMkacOGDfL397eFX0mKjo6Wm5ubNm7caOtz//33y8PDw9YnJiZG+/bt0z///JPvtuPi4uTn52e7hYaGFsUuAgAAwAlcNgAnJSVJkoKCguzag4KCbMuSkpIUGBhot9zd3V0BAQF2ffJbx6XbuNyQIUOUmppqux07duzGdwgAAAAuwalTIFyV1WqV1Wp1dhkAAAAoAi47AhwcHCxJSk5OtmtPTk62LQsODtapU6fsll+4cEEpKSl2ffJbx6XbAAAAgHm4bACOiIhQcHCwVq1aZWtLS0vTxo0bFRUVJUmKiorSmTNntHXrVluf1atXKycnRw0bNrT1Wbt2rbKysmx9VqxYoXvuuUelSpW6SXsDAAAAV+HUAHzu3DklJCQoISFB0sUvviUkJCgxMVEWi0UDBgzQ6NGjtXTpUu3cuVPdunVTSEiIOnToIEmqUqWKWrZsqT59+mjTpk365Zdf1K9fP3Xp0kUhISGSpCeffFIeHh7q3bu3du/erQULFmjixIkaOHCgk/YaAAAAzuTUOcBbtmxRs2bNbPdzQ2n37t01c+ZMvfrqq0pPT1ffvn115swZNW7cWPHx8fL09LQ9Zs6cOerXr5+aN28uNzc3dezYUZMmTbIt9/Pz0/LlyxUbG6u6deuqTJkyGjZs2BUvgQYAAIDbm8UwDMPZRbi6tLQ0+fn5KTU1Vb6+vkW6rXe3ny7S9d9sr9Uu4+wSHOp2Oj4cG9d2ux0fAChq15PXXHYOMAAAAFAUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATMXd2QUAAG5v724/7ewSHOq12mWcXQKAG8QIMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEzF3dkFAAAA53l3+2lnl+Awr9Uu4+wScItgBBgAAACmQgAGAACAqRCAAQAAYCoEYAAAAJgKARgAAACm4tIBODs7W2+++aYiIiLk5eWlihUratSoUTIMw9bHMAwNGzZM5cqVk5eXl6Kjo3XgwAG79aSkpKhr167y9fWVv7+/evfurXPnzt3s3QEAAIALcOkA/N577+njjz/Whx9+qL179+q9997TmDFjNHnyZFufMWPGaNKkSZo6dao2btwob29vxcTE6Pz587Y+Xbt21e7du7VixQotW7ZMa9euVd++fZ2xSwAAAHAyl74O8Pr169W+fXu1adNGkhQeHq558+Zp06ZNki6O/k6YMEFDhw5V+/btJUmzZ89WUFCQlixZoi5dumjv3r2Kj4/X5s2bVa9ePUnS5MmT1bp1a40dO1YhISHO2TkAAAA4hUuPAN93331atWqV9u/fL0n67bfftG7dOrVq1UqSdOTIESUlJSk6Otr2GD8/PzVs2FAbNmyQJG3YsEH+/v628CtJ0dHRcnNz08aNG/PdbkZGhtLS0uxuAAAAuD249Ajwa6+9prS0NFWuXFnFihVTdna23n77bXXt2lWSlJSUJEkKCgqye1xQUJBtWVJSkgIDA+2Wu7u7KyAgwNbncnFxcXrrrbccvTsAAABwAS49AvzFF19ozpw5mjt3rrZt26ZZs2Zp7NixmjVrVpFud8iQIUpNTbXdjh07VqTbAwAAwM3j0iPAgwYN0muvvaYuXbpIkqpXr66jR48qLi5O3bt3V3BwsCQpOTlZ5cqVsz0uOTlZtWrVkiQFBwfr1KlTduu9cOGCUlJSbI+/nNVqldVqLYI9AgAAgLO59Ajwv//+Kzc3+xKLFSumnJwcSVJERISCg4O1atUq2/K0tDRt3LhRUVFRkqSoqCidOXNGW7dutfVZvXq1cnJy1LBhw5uwFwAAAHAlLj0C3K5dO7399tsqX768qlatqu3bt2v8+PHq1auXJMlisWjAgAEaPXq0KlWqpIiICL355psKCQlRhw4dJElVqlRRy5Yt1adPH02dOlVZWVnq16+funTpwhUgAAAATMilA/DkyZP15ptv6vnnn9epU6cUEhKiZ599VsOGDbP1efXVV5Wenq6+ffvqzJkzaty4seLj4+Xp6WnrM2fOHPXr10/NmzeXm5ubOnbsqEmTJjljlwAAAOBkLh2AfXx8NGHCBE2YMOGKfSwWi0aOHKmRI0desU9AQIDmzp1bBBUCAADgVuPSc4ABAAAARyMAAwAAwFQIwAAAADCVQgXgw4cPO7oOAAAA4KYoVAC+66671KxZM33++ec6f/68o2sCAAAAikyhAvC2bdtUo0YNDRw4UMHBwXr22We1adMmR9cGAAAAOFyhAnCtWrU0ceJEnThxQp9++qlOnjypxo0bq1q1aho/frz++usvR9cJAAAAOMQNfQnO3d1djz76qBYuXKj33ntPBw8e1CuvvKLQ0FB169ZNJ0+edFSdAAAAgEPcUADesmWLnn/+eZUrV07jx4/XK6+8okOHDmnFihU6ceKE2rdv76g6AQAAAIco1C/BjR8/XjNmzNC+ffvUunVrzZ49W61bt5ab28U8HRERoZkzZyo8PNyRtQIAAAA3rFAB+OOPP1avXr3Uo0cPlStXLt8+gYGBmj59+g0VBwAAADhaoQLwgQMHrtnHw8ND3bt3L8zqAQAAgCJTqDnAM2bM0MKFC/O0L1y4ULNmzbrhogAAAICiUqgAHBcXpzJlyuRpDwwM1DvvvHPDRQEAAABFpVABODExUREREXnaw8LClJiYeMNFAQAAAEWlUAE4MDBQO3bsyNP+22+/qXTp0jdcFAAAAFBUChWAn3jiCfXv319r1qxRdna2srOztXr1ar344ovq0qWLo2sEAAAAHKZQV4EYNWqU/vjjDzVv3lzu7hdXkZOTo27dujEHGAAAAC6tUAHYw8NDCxYs0KhRo/Tbb7/Jy8tL1atXV1hYmKPrAwAAAByqUAE419133627777bUbUAAAAARa5QATg7O1szZ87UqlWrdOrUKeXk5NgtX716tUOKAwAAABytUAH4xRdf1MyZM9WmTRtVq1ZNFovF0XUBAAAARaJQAXj+/Pn64osv1Lp1a0fXAwAAABSpQl0GzcPDQ3fddZejawEAAACKXKEC8Msvv6yJEyfKMAxH1wMAAAAUqUJNgVi3bp3WrFmj77//XlWrVlXx4sXtln/11VcOKQ4AAABwtEIFYH9/fz3yyCOOrgUAAAAocoUKwDNmzHB0HQAAAMBNUag5wJJ04cIFrVy5Up988onOnj0rSTpx4oTOnTvnsOIAAAAARyvUCPDRo0fVsmVLJSYmKiMjQw899JB8fHz03nvvKSMjQ1OnTnV0nQAAAIBDFGoE+MUXX1S9evX0zz//yMvLy9b+yCOPaNWqVQ4rDgAAAHC0Qo0A//zzz1q/fr08PDzs2sPDw3X8+HGHFAYAAAAUhUKNAOfk5Cg7OztP+59//ikfH58bLgoAAAAoKoUKwC1atNCECRNs9y0Wi86dO6fhw4fz88gAAABwaYWaAjFu3DjFxMQoMjJS58+f15NPPqkDBw6oTJkymjdvnqNrBAAAABymUAH4zjvv1G+//ab58+drx44dOnfunHr37q2uXbvafSkOAAAAcDWFCsCS5O7urqeeesqRtQAAAABFrlABePbs2Vdd3q1bt0IVAwAAABS1QgXgF1980e5+VlaW/v33X3l4eKhEiRIEYAAAALisQl0F4p9//rG7nTt3Tvv27VPjxo35EhwAAABcWqECcH4qVaqkd999N8/oMAAAAOBKHBaApYtfjDtx4oQjVwkAAAA4VKHmAC9dutTuvmEYOnnypD788EM1atTIIYUBAAAARaFQAbhDhw529y0Wi8qWLasHH3xQ48aNc0RdAAAAQJEoVADOyclxdB0AAADATeHQOcAAAACAqyvUCPDAgQML3Hf8+PGF2QQAAABQJAoVgLdv367t27crKytL99xzjyRp//79KlasmOrUqWPrZ7FYHFMlAAAA4CCFmgLRrl073X///frzzz+1bds2bdu2TceOHVOzZs3Utm1brVmzRmvWrNHq1atvuMDjx4/rqaeeUunSpeXl5aXq1atry5YttuWGYWjYsGEqV66cvLy8FB0drQMHDtitIyUlRV27dpWvr6/8/f3Vu3dvnTt37oZrAwAAwK2nUAF43LhxiouLU6lSpWxtpUqV0ujRox16FYh//vlHjRo1UvHixfX9999rz549GjdunN12x4wZo0mTJmnq1KnauHGjvL29FRMTo/Pnz9v6dO3aVbt379aKFSu0bNkyrV27Vn379nVYnQAAALh1FGoKRFpamv7666887X/99ZfOnj17w0Xleu+99xQaGqoZM2bY2iIiImz/NgxDEyZM0NChQ9W+fXtJ0uzZsxUUFKQlS5aoS5cu2rt3r+Lj47V582bVq1dPkjR58mS1bt1aY8eOVUhIiMPqBQAAgOsr1AjwI488op49e+qrr77Sn3/+qT///FNffvmlevfurUcffdRhxS1dulT16tXTY489psDAQNWuXVv//e9/bcuPHDmipKQkRUdH29r8/PzUsGFDbdiwQZK0YcMG+fv728KvJEVHR8vNzU0bN27Md7sZGRlKS0uzuwEAAOD2UKgAPHXqVLVq1UpPPvmkwsLCFBYWpieffFItW7bUlClTHFbc4cOH9fHHH6tSpUr64Ycf9Nxzz6l///6aNWuWJCkpKUmSFBQUZPe4oKAg27KkpCQFBgbaLXd3d1dAQICtz+Xi4uLk5+dnu4WGhjpsnwAAAOBchZoCUaJECU2ZMkXvv/++Dh06JEmqWLGivL29HVpcTk6O6tWrp3feeUeSVLt2be3atUtTp05V9+7dHbqtSw0ZMsTuUm9paWmEYAAAgNvEDf0QxsmTJ3Xy5ElVqlRJ3t7eMgzDUXVJksqVK6fIyEi7tipVqigxMVGSFBwcLElKTk6265OcnGxbFhwcrFOnTtktv3DhglJSUmx9Lme1WuXr62t3AwAAwO2hUAH477//VvPmzXX33XerdevWOnnypCSpd+/eevnllx1WXKNGjbRv3z67tv379yssLEzSxS/EBQcHa9WqVbblaWlp2rhxo6KioiRJUVFROnPmjLZu3Wrrs3r1auXk5Khhw4YOqxUAAAC3hkIF4JdeeknFixdXYmKiSpQoYWvv3Lmz4uPjHVbcSy+9pF9//VXvvPOODh48qLlz52ratGmKjY2VdPGHNgYMGKDRo0dr6dKl2rlzp7p166aQkBB16NBB0sUR45YtW6pPnz7atGmTfvnlF/Xr109dunThChAAAAAmVKg5wMuXL9cPP/ygO++80669UqVKOnr0qEMKk6T69etr8eLFGjJkiEaOHKmIiAhNmDBBXbt2tfV59dVXlZ6err59++rMmTNq3Lix4uPj5enpaeszZ84c9evXT82bN5ebm5s6duyoSZMmOaxOAAAA3DoKFYDT09PtRn5zpaSkyGq13nBRl2rbtq3atm17xeUWi0UjR47UyJEjr9gnICBAc+fOdWhdAAAAuDUVagpEkyZNNHv2bNt9i8WinJwcjRkzRs2aNXNYcQAAAICjFWoEeMyYMWrevLm2bNmizMxMvfrqq9q9e7dSUlL0yy+/OLpGAAAAwGEKNQJcrVo17d+/X40bN1b79u2Vnp6uRx99VNu3b1fFihUdXSMAAADgMNc9ApyVlaWWLVtq6tSpeuONN4qiJgAAAKDIXPcIcPHixbVjx46iqAUAAAAocoWaAvHUU09p+vTpjq4FAAAAKHKF+hLchQsX9Omnn2rlypWqW7euvL297ZaPHz/eIcUBAAAAjnZdAfjw4cMKDw/Xrl27VKdOHUkXf5r4UhaLxXHVAQAAAA52XQG4UqVKOnnypNasWSPp4k8fT5o0SUFBQUVSHAAAAOBo1zUH2DAMu/vff/+90tPTHVoQAAAAUJQK9SW4XJcHYgAAAMDVXVcAtlgseeb4MucXAAAAt5LrmgNsGIZ69Oghq9UqSTp//rz+85//5LkKxFdffeW4CgEAAAAHuq4A3L17d7v7Tz31lEOLAQAAAIradQXgGTNmFFUdAAAAwE1xQ1+CAwAAAG41BGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAqBGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAqBGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAqBGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAqBGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAqBGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAqBGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAqBGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAqt1QAfvfdd2WxWDRgwABb2/nz5xUbG6vSpUurZMmS6tixo5KTk+0el5iYqDZt2qhEiRIKDAzUoEGDdOHChZtcPQAAAFzBLROAN2/erE8++UQ1atSwa3/ppZf0zTffaOHChfrpp5904sQJPfroo7bl2dnZatOmjTIzM7V+/XrNmjVLM2fO1LBhw272LgAAAMAF3BIB+Ny5c+ratav++9//qlSpUrb21NRUTZ8+XePHj9eDDz6ounXrasaMGVq/fr1+/fVXSdLy5cu1Z88eff7556pVq5ZatWqlUaNG6aOPPlJmZma+28vIyFBaWprdDQAAALeHWyIAx8bGqk2bNoqOjrZr37p1q7KysuzaK1eurPLly2vDhg2SpA0bNqh69eoKCgqy9YmJiVFaWpp2796d7/bi4uLk5+dnu4WGhhbBXgEAAMAZXD4Az58/X9u2bVNcXFyeZUlJSfLw8JC/v79de1BQkJKSkmx9Lg2/uctzl+VnyJAhSk1Ntd2OHTvmgD0BAACAK3B3dgFXc+zYMb344otasWKFPD09b9p2rVarrFbrTdseAAAAbh6XHgHeunWrTp06pTp16sjd3V3u7u766aefNGnSJLm7uysoKEiZmZk6c+aM3eOSk5MVHBwsSQoODs5zVYjc+7l9AAAAYB4uHYCbN2+unTt3KiEhwXarV6+eunbtavt38eLFtWrVKttj9u3bp8TEREVFRUmSoqKitHPnTp06dcrWZ8WKFfL19VVkZORN3ycAAAA4l0tPgfDx8VG1atXs2ry9vVW6dGlbe+/evTVw4EAFBATI19dXL7zwgqKionTvvfdKklq0aKHIyEg9/fTTGjNmjJKSkjR06FDFxsYyzQEAAMCEXDoAF8QHH3wgNzc3dezYURkZGYqJidGUKVNsy4sVK6Zly5bpueeeU1RUlLy9vdW9e3eNHDnSiVUDAADAWW65APzjjz/a3ff09NRHH32kjz766IqPCQsL03fffVfElQEAAOBW4NJzgAEAAABHIwADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBUCMAAAAEyFAAwAAABTIQADAADAVAjAAAAAMBWXDsBxcXGqX7++fHx8FBgYqA4dOmjfvn12fc6fP6/Y2FiVLl1aJUuWVMeOHZWcnGzXJzExUW3atFGJEiUUGBioQYMG6cKFCzdzVwAAAOAiXDoA//TTT4qNjdWvv/6qFStWKCsrSy1atFB6erqtz0svvaRvvvlGCxcu1E8//aQTJ07o0UcftS3Pzs5WmzZtlJmZqfXr12vWrFmaOXOmhg0b5oxdAgAAgJO5O7uAq4mPj7e7P3PmTAUGBmrr1q26//77lZqaqunTp2vu3Ll68MEHJUkzZsxQlSpV9Ouvv+ree+/V8uXLtWfPHq1cuVJBQUGqVauWRo0apcGDB2vEiBHy8PBwxq4BAADASVx6BPhyqampkqSAgABJ0tatW5WVlaXo6Ghbn8qVK6t8+fLasGGDJGnDhg2qXr26goKCbH1iYmKUlpam3bt357udjIwMpaWl2d0AAABwe7hlAnBOTo4GDBigRo0aqVq1apKkpKQkeXh4yN/f365vUFCQkpKSbH0uDb+5y3OX5ScuLk5+fn62W2hoqIP3BgAAAM5yywTg2NhY7dq1S/Pnzy/ybQ0ZMkSpqam227Fjx4p8mwAAALg5XHoOcK5+/fpp2bJlWrt2re68805be3BwsDIzM3XmzBm7UeDk5GQFBwfb+mzatMlufblXicjtczmr1Sqr1ergvQAAAIArcOkRYMMw1K9fPy1evFirV69WRESE3fK6deuqePHiWrVqla1t3759SkxMVFRUlCQpKipKO3fu1KlTp2x9VqxYIV9fX0VGRt6cHQEAAIDLcOkR4NjYWM2dO1dff/21fHx8bHN2/fz85OXlJT8/P/Xu3VsDBw5UQECAfH199cILLygqKkr33nuvJKlFixaKjIzU008/rTFjxigpKUlDhw5VbGwso7wAAAAm5NIB+OOPP5YkNW3a1K59xowZ6tGjhyTpgw8+kJubmzp27KiMjAzFxMRoypQptr7FihXTsmXL9NxzzykqKkre3t7q3r27Ro4cebN2AwAAAC7EpQOwYRjX7OPp6amPPvpIH3300RX7hIWF6bvvvnNkaQAAALhFufQcYAAAAMDRCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFXdnFwAAAIC83t1+2tklONRrtcs4uwQbRoABAABgKgRgAAAAmAoBGAAAAKZCAAYAAICpEIABAABgKgRgAAAAmAoBGAAAAKZCAAYAAICpEIABAABgKgRgAAAAmAoBGAAAAKZCAAYAAICpEIABAABgKgRgAAAAmAoBGAAAAKZCAAYAAICpEIABAABgKgRgAAAAmAoBGAAAAKZCAAYAAICpEIABAABgKqYKwB999JHCw8Pl6emphg0batOmTc4uCQAAADeZaQLwggULNHDgQA0fPlzbtm1TzZo1FRMTo1OnTjm7NAAAANxEpgnA48ePV58+fdSzZ09FRkZq6tSpKlGihD799FNnlwYAAICbyN3ZBdwMmZmZ2rp1q4YMGWJrc3NzU3R0tDZs2JCnf0ZGhjIyMmz3U1NTJUlpaWlFXuv5c2eLfBs3U1qah7NLcKjb6fhwbFzb7XR8ODau7XY6Phwb11bUxyc3pxmGcc2+pgjAp0+fVnZ2toKCguzag4KC9Pvvv+fpHxcXp7feeitPe2hoaJHVeLvK+yzCVXBsXBvHx3VxbFwXx8a13azjc/bsWfn5+V21jykC8PUaMmSIBg4caLufk5OjlJQUlS5dWhaLxYmVOUZaWppCQ0N17Ngx+fr6OrscXIJj49o4Pq6LY+O6ODau7XY6PoZh6OzZswoJCblmX1ME4DJlyqhYsWJKTk62a09OTlZwcHCe/larVVar1a7N39+/KEt0Cl9f31v+ZL9dcWxcG8fHdXFsXBfHxrXdLsfnWiO/uUzxJTgPDw/VrVtXq1atsrXl5ORo1apVioqKcmJlAAAAuNlMMQIsSQMHDlT37t1Vr149NWjQQBMmTFB6erp69uzp7NIAAABwE5kmAHfu3Fl//fWXhg0bpqSkJNWqVUvx8fF5vhhnBlarVcOHD88zzQPOx7FxbRwf18WxcV0cG9dm1uNjMQpyrQgAAADgNmGKOcAAAABALgIwAAAATIUADAAAAFMhAAMAAMBUCMAAAAAwFQIwABQAF8xxTdnZ2ZI4Pq7o5MmT2rNnj7PLQD5ycnJsrx2zIgCbhNlPdFeWkpKi33//XQcOHFBmZqazy8El0tPTdfbsWaWlpclisTi7HFwmISFBHTp00L///svxcTHHjx9X9erVNXToUG3ZssXZ5eASe/bsUbdu3RQTE6PnnntO69evd3ZJTkEANoH9+/drwoQJOnnypLNLwWV27dql6OhoPf7446pevbrGjBnDHysuYs+ePXr00Uf1wAMPqEqVKpozZ44kRhpdxW+//ab77rtPVatWVYkSJWztHB/XcODAAaWmpio1NVWTJ0/Wtm3bbMs4Rs6zb98+3XfffcrOzlb9+vW1YcMGvfjii5o0aZKzS7vpCMC3uYMHDyoqKkqDBg3S5MmTdfr0aWeXhP9nz549atq0qZo3b6758+fr7bff1rBhw3TixAlnl2Z6e/bs0f3336+qVavqlVdeUZcuXdSzZ08lJCQw0ugCduzYoUaNGqlfv3569913be2ZmZkcHxdRo0YNtW7dWp07d9auXbs0fvx47d69WxIB2FkMw9Ds2bMVExOjefPmKS4uTj///LM6dOigGTNmaMyYMc4u8abil+BuY+np6erfv79ycnJUv3599evXT6+88opeffVVlSlTxtnlmdrp06fVsWNH1a5dWxMmTJB08c2pdevWGjZsmLy8vFS6dGmFhoY6t1ATSklJ0RNPPKHKlStr4sSJtvZmzZqpevXqmjRpkgzDIGg5SVJSkmrXrq2aNWsqPj5e2dnZeuWVV3TgwAEdOnRIzz77rFq2bKnKlSs7u1TTys7OVkpKiho3bqzVq1dr06ZNiouLU61atbR7926VK1dOixYtcnaZptSzZ08dPnxYP/30k63t7NmzmjZtmubPn68BAwaoa9euTqzw5nF3dgEoOm5ubqpbt65Kly6tzp07q0yZMurSpYskEYKdzGKxqGXLlurUqZOtbfTo0frhhx+UlJSk06dPq2rVqho6dKgaN27sxErNJysrS2fOnLEdm5ycHLm5uSkiIkIpKSmSRPh1sqioKB07dkxff/21pk6dqqysLNWqVUvh4eGaNGmSdu3apWHDhql8+fLOLtWU3NzcVLZsWdWvX1+7du3SI488IqvVqu7duysjI0N9+vRxdommk/tHe506dXTgwAHt27dP99xzjyTJx8dHvXr10r59+zRlyhQ98sgjdtOKblsGbmvnzp2zuz9//nzDYrEYr7zyinH69GnDMAwjOzvbOHz4sDPKM7W0tDTbv+fNm2dYLBZjwYIFxt9//2389NNPRv369Y0RI0Y4sULz2r9/v+3fmZmZhmEYxtChQ42nn37art/Zs2dval246MSJE0a3bt0MLy8v46GHHrK9lxmGYcyZM8fw9/c3vvvuOydWCMMwjG7duhmvvfaaYRiG0bt3b6NUqVJGZGSk0atXL2Pjxo1Ors6cDh48aJQpU8bo1auX7f0rJyfHMAzDSExMNCwWi/H99987s8SbhhHg25y3t7ekix9Jubm5qXPnzjIMQ08++aQsFosGDBigsWPH6ujRo/rss8/M8Vefi/Dx8bH9OyoqSlu2bFGdOnUkSffff78CAwO1detWZ5VnapUqVZJ0cfS3ePHiki6OoJw6dcrWJy4uTlarVf3795e7O2+lN1O5cuUUFxenO+64Q9HR0SpdurRthOvJJ5/U8OHDtWbNGrVq1crZpZpS7rF48MEHdeTIET3//PP67rvvtHXrViUkJGjQoEHy8PBQjRo15Onp6exyTaVixYr64osv1KpVK3l5eWnEiBG2T4OLFy+uGjVqyM/Pz8lV3hy8a5tEsWLFZBiGcnJy1KVLF1ksFj399NNaunSpDh06pM2bNxN+nSgsLExhYWGSLoauzMxMlSxZUjVq1HByZebm5uZmN9/Xze3i94aHDRum0aNHa/v27YRfJwkJCdFrr71mC1AWi0WGYSglJUVly5ZVrVq1nFugieW+XiIiItSzZ08FBQVp2bJlioiIUEREhCwWi2rWrEn4dZJmzZpp4cKFeuyxx3Ty5Ek9/vjjqlGjhmbPnq1Tp06Z5rsnfAnOZHIPt8ViUfPmzZWQkKAff/xR1atXd3JluNSwYcM0a9YsrVy50jYaCefInQM8YsQInTx5UpUqVdLQoUO1fv1624g9XMfw4cM1b948rVixwvZHJZwjKytLn332merVq6caNWrw5VEXs23bNg0cOFB//PGH3N3dVaxYMc2fP1+1a9d2dmk3BUMXJmOxWJSdna1BgwZpzZo1SkhIIPy6kIULF+qnn37S/PnztWLFCsKvC8gd9S1evLj++9//ytfXV+vWrSP8upj58+drzZo1WrhwoVatWkX4dQHFixdXjx49bK8hwq9rqVOnjpYuXaqUlBSdPXtW5cqVM9WX47kOsElVrVpV27Zt4yN2FxMZGam//vpLP//8s2n+Cr9VxMTESJLWr1+vevXqObkaXC4yMlLHjx/nteNicsMvXJOvr6/Cw8NVvXp1U4VfiSkQpsVHUa4rKyvL9sUruJb09HTbF0vhejIzM+Xh4eHsMgDcAgjAAAAAMBU+mwAAAICpEIABAABgKgRgAAAAmAoBGAAAAKZCAAYAAICpEIABAABgKgRgALiNWSwWLVmyxNllAIBLIQADwC0sKSlJL7zwgipUqCCr1arQ0FC1a9dOq1atcnZpAOCy3J1dAACgcP744w81atRI/v7+ev/991W9enVlZWXphx9+UGxsrH7//XdnlwgALokRYAC4RT3//POyWCzatGmTOnbsqLvvvltVq1bVwIED9euvv+b7mMGDB+vuu+9WiRIlVKFCBb355pvKysqyLf/tt9/UrFkz+fj4yNfXV3Xr1tWWLVskSUePHlW7du1UqlQpeXt7q2rVqvruu+9uyr4CgCMxAgwAt6CUlBTFx8fr7bfflre3d57l/v7++T7Ox8dHM2fOVEhIiHbu3Kk+ffrIx8dHr776qiSpa9euql27tj7++GMVK1ZMCQkJKl68uCQpNjZWmZmZWrt2rby9vbVnzx6VLFmyyPYRAIoKARgAbkEHDx6UYRiqXLnydT1u6NChtn+Hh4frlVde0fz5820BODExUYMGDbKtt1KlSrb+iYmJ6tixo6pXry5JqlChwo3uBgA4BVMgAOAWZBhGoR63YMECNWrUSMHBwSpZsqSGDh2qxMRE2/KBAwfqmWeeUXR0tN59910dOnTItqx///4aPXq0GjVqpOHDh2vHjh03vB8A4AwEYAC4BVWqVEkWi+W6vui2YcMGde3aVa1bt9ayZcu0fft2vfHGG8rMzLT1GTFihHbv3q02bdpo9erVioyM1OLFiyVJzzzzjA4fPqynn35aO3fuVL169TR58mSH7xsAFDWLUdhhBACAU7Vq1Uo7d+7Uvn378swDPnPmjPz9/WWxWLR48WJ16NBB48aN05QpU+xGdZ955hktWrRIZ86cyXcbTzzxhNLT07V06dI8y4YMGaJvv/2WkWAAtxxGgAHgFvXRRx8pOztbDRo00JdffqkDBw5o7969mjRpkqKiovL0r1SpkhITEzV//nwdOnRIkyZNso3uStL//vc/9evXTz/++KOOHj2qX375RZs3b1aVKlUkSQMGDNAPP/ygI0eOaNu2bVqzZo1tGQDcSvgSHADcoipUqKBt27bp7bff1ssvv6yTJ0+qbNmyqlu3rj7++OM8/R9++GG99NJL6tevnzIyMtSmTRu9+eabGjFihCSpWLFi+vvvv9WtWzclJyerTJkyevTRR/XWW29JkrKzsxUbG6s///xTvr6+atmypT744IObucsA4BBMgQAAAICpMAUCAAAApkIABgAAgKkQgAEAAGAqBGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAqBGAAAACYCgEYAAAApkIABgAAgKkQgAEAAGAq/x99HsFXLnjheQAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# load the TREC dataset\n",
        "dataset = load_dataset('trec')\n",
        "\n",
        "train_data = dataset['train']\n",
        "test_data = dataset['test']\n",
        "\n",
        "train_df = pd.DataFrame(train_data)\n",
        "test_df = pd.DataFrame(test_data)\n",
        "\n",
        "print(f\"Training samples: {len(train_df)}\")\n",
        "print(f\"Test samples: {len(test_df)}\")\n",
        "\n",
        "print(train_df.head())\n",
        "\n",
        "# we will be using coarse labels : 6 classes\n",
        "class_distribution = train_df['coarse_label'].value_counts()\n",
        "print(\"\\nClass distribution (Coarse Labels):\")\n",
        "print(class_distribution)\n",
        "\n",
        "\n",
        "# Plot the distribution of coarse labels\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "class_distribution.plot(kind='bar', color='skyblue')\n",
        "plt.title('Class Distribution (Coarse Labels) in Training Set')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Frequency')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()\n",
        "# class zero is relatively undersampled, we'll use a weighted sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "evx8Hf4Dl8XN",
        "outputId": "847defce-357d-4426-ca16-58a995d7e2bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.4081\n",
            "Epoch 2, Loss: 0.1159\n",
            "Epoch 3, Loss: 0.0718\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['test', 'train']\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-a84131af2164>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# no weight update during validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/dataset_dict.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, k)\u001b[0m\n\u001b[1;32m     76\u001b[0m             ]\n\u001b[1;32m     77\u001b[0m             \u001b[0msuggested_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mavailable_suggested_splits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mavailable_suggested_splits\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m     79\u001b[0m                 \u001b[0;34mf\"Invalid key: {k}. Please first select a split. For example: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;34mf\"`my_dataset_dictionary['{suggested_split}'][{k}]`. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"Invalid key: 0. Please first select a split. For example: `my_dataset_dictionary['train'][0]`. Available splits: ['test', 'train']\""
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data import WeightedRandomSampler\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# dataset class\n",
        "class TrecDataset(Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6)  # 6 coarse labels\n",
        "\n",
        "# text : coarse label : fine label\n",
        "texts = [item['text'] for item in train_data]\n",
        "coarse_labels = [item['coarse_label'] for item in train_data]\n",
        "\n",
        "texts_test = [item['text'] for item in test_data]\n",
        "coarse_labels_test = [item['coarse_label'] for item in test_data]\n",
        "\n",
        "# Tokenize the data\n",
        "encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
        "encodings_test = tokenizer(texts_test, truncation=True, padding=True, max_length=128)\n",
        "\n",
        "\n",
        "train = TrecDataset(encodings, coarse_labels)\n",
        "test = TrecDataset(encodings_test, coarse_labels_test)\n",
        "\n",
        "# Calculate class weights for imbalance handling\n",
        "label_counts = np.bincount(coarse_labels)\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(coarse_labels), y=coarse_labels)\n",
        "sample_weights = [class_weights[label] for label in coarse_labels]\n",
        "\n",
        "# Define the sampler for oversampling\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "\n",
        "train_loader = DataLoader(train, batch_size=16, sampler=sampler)\n",
        "test_loader = DataLoader(test, batch_size=16)\n",
        "\n",
        "# pretrained bert here includes a classification headwhich classifies CLS tokens to the number of classes.\n",
        "# Training loop\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "for epoch in range(3):\n",
        "   # set to training mode\n",
        "    model.train()\n",
        "    total_loss = 0 # total loss for current epoch\n",
        "    for batch in train_loader:\n",
        "      # clear gradients from previous batch\n",
        "        optimizer.zero_grad()\n",
        "      # move data\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "      #input to model, fwd pass\n",
        "      #outputs loss and logits\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        loss.backward() # backprpagation\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad(): # no weight update during validation\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Training Accuracy: {correct / total}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MsxkIFKgbtgo",
        "outputId": "183d4ebf-82b1-4a99-8a85-fe5a0688adde"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        },
        "id": "dCuMxxjMe2sK",
        "outputId": "3217c47a-1aba-44ba-a10e-dd2f2441715f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of  Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1023' max='1023' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1023/1023 02:18, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.396073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.003900</td>\n",
              "      <td>0.287741</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.006400</td>\n",
              "      <td>0.275794</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1023, training_loss=0.005035457347077249, metrics={'train_runtime': 138.4318, 'train_samples_per_second': 118.152, 'train_steps_per_second': 7.39, 'total_flos': 344624136832656.0, 'train_loss': 0.005035457347077249, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# fine tuning : you can specify if all weights can be trainable / what to freeze / what to train\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# configure training process\n",
        "# freeze or choose params to train\n",
        "for name, param in model.named_parameters():\n",
        "    if \"encoder.layer\" in name:\n",
        "        # Extract layer number correctly using regex\n",
        "        import re\n",
        "        match = re.search(r\"encoder\\.layer\\.(\\d+)\", name)\n",
        "        if match:\n",
        "            layer_number = int(match.group(1))\n",
        "            if layer_number < 6:  # Freeze the first 6 layers\n",
        "                param.requires_grad = False\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "        output_dir='./dot',\n",
        "        num_train_epochs=3,\n",
        "        per_device_train_batch_size=16,\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        logging_dir='./logs',\n",
        "    )\n",
        "# pass the processed data, the one with input ids and encodings\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset= train,\n",
        "    eval_dataset = test\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JiE_c2n_fGg8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70b5af7f-fe87-44a3-c2b7-d3dd2362f43a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Accuracy: 0.972\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "print(f\"Training Accuracy: {correct / total}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "u8pZ0sO5s3b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1029f233-d4dc-4a41-f911-e620f1a96258"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert.embeddings.word_embeddings.weight True\n",
            "bert.embeddings.position_embeddings.weight True\n",
            "bert.embeddings.token_type_embeddings.weight True\n",
            "bert.embeddings.LayerNorm.weight True\n",
            "bert.embeddings.LayerNorm.bias True\n",
            "bert.encoder.layer.0.attention.self.query.weight False\n",
            "bert.encoder.layer.0.attention.self.query.bias False\n",
            "bert.encoder.layer.0.attention.self.key.weight False\n",
            "bert.encoder.layer.0.attention.self.key.bias False\n",
            "bert.encoder.layer.0.attention.self.value.weight False\n",
            "bert.encoder.layer.0.attention.self.value.bias False\n",
            "bert.encoder.layer.0.attention.output.dense.weight False\n",
            "bert.encoder.layer.0.attention.output.dense.bias False\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.0.intermediate.dense.weight False\n",
            "bert.encoder.layer.0.intermediate.dense.bias False\n",
            "bert.encoder.layer.0.output.dense.weight False\n",
            "bert.encoder.layer.0.output.dense.bias False\n",
            "bert.encoder.layer.0.output.LayerNorm.weight False\n",
            "bert.encoder.layer.0.output.LayerNorm.bias False\n",
            "bert.encoder.layer.1.attention.self.query.weight False\n",
            "bert.encoder.layer.1.attention.self.query.bias False\n",
            "bert.encoder.layer.1.attention.self.key.weight False\n",
            "bert.encoder.layer.1.attention.self.key.bias False\n",
            "bert.encoder.layer.1.attention.self.value.weight False\n",
            "bert.encoder.layer.1.attention.self.value.bias False\n",
            "bert.encoder.layer.1.attention.output.dense.weight False\n",
            "bert.encoder.layer.1.attention.output.dense.bias False\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.1.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.1.intermediate.dense.weight False\n",
            "bert.encoder.layer.1.intermediate.dense.bias False\n",
            "bert.encoder.layer.1.output.dense.weight False\n",
            "bert.encoder.layer.1.output.dense.bias False\n",
            "bert.encoder.layer.1.output.LayerNorm.weight False\n",
            "bert.encoder.layer.1.output.LayerNorm.bias False\n",
            "bert.encoder.layer.2.attention.self.query.weight False\n",
            "bert.encoder.layer.2.attention.self.query.bias False\n",
            "bert.encoder.layer.2.attention.self.key.weight False\n",
            "bert.encoder.layer.2.attention.self.key.bias False\n",
            "bert.encoder.layer.2.attention.self.value.weight False\n",
            "bert.encoder.layer.2.attention.self.value.bias False\n",
            "bert.encoder.layer.2.attention.output.dense.weight False\n",
            "bert.encoder.layer.2.attention.output.dense.bias False\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.2.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.2.intermediate.dense.weight False\n",
            "bert.encoder.layer.2.intermediate.dense.bias False\n",
            "bert.encoder.layer.2.output.dense.weight False\n",
            "bert.encoder.layer.2.output.dense.bias False\n",
            "bert.encoder.layer.2.output.LayerNorm.weight False\n",
            "bert.encoder.layer.2.output.LayerNorm.bias False\n",
            "bert.encoder.layer.3.attention.self.query.weight False\n",
            "bert.encoder.layer.3.attention.self.query.bias False\n",
            "bert.encoder.layer.3.attention.self.key.weight False\n",
            "bert.encoder.layer.3.attention.self.key.bias False\n",
            "bert.encoder.layer.3.attention.self.value.weight False\n",
            "bert.encoder.layer.3.attention.self.value.bias False\n",
            "bert.encoder.layer.3.attention.output.dense.weight False\n",
            "bert.encoder.layer.3.attention.output.dense.bias False\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.3.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.3.intermediate.dense.weight False\n",
            "bert.encoder.layer.3.intermediate.dense.bias False\n",
            "bert.encoder.layer.3.output.dense.weight False\n",
            "bert.encoder.layer.3.output.dense.bias False\n",
            "bert.encoder.layer.3.output.LayerNorm.weight False\n",
            "bert.encoder.layer.3.output.LayerNorm.bias False\n",
            "bert.encoder.layer.4.attention.self.query.weight False\n",
            "bert.encoder.layer.4.attention.self.query.bias False\n",
            "bert.encoder.layer.4.attention.self.key.weight False\n",
            "bert.encoder.layer.4.attention.self.key.bias False\n",
            "bert.encoder.layer.4.attention.self.value.weight False\n",
            "bert.encoder.layer.4.attention.self.value.bias False\n",
            "bert.encoder.layer.4.attention.output.dense.weight False\n",
            "bert.encoder.layer.4.attention.output.dense.bias False\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.4.intermediate.dense.weight False\n",
            "bert.encoder.layer.4.intermediate.dense.bias False\n",
            "bert.encoder.layer.4.output.dense.weight False\n",
            "bert.encoder.layer.4.output.dense.bias False\n",
            "bert.encoder.layer.4.output.LayerNorm.weight False\n",
            "bert.encoder.layer.4.output.LayerNorm.bias False\n",
            "bert.encoder.layer.5.attention.self.query.weight False\n",
            "bert.encoder.layer.5.attention.self.query.bias False\n",
            "bert.encoder.layer.5.attention.self.key.weight False\n",
            "bert.encoder.layer.5.attention.self.key.bias False\n",
            "bert.encoder.layer.5.attention.self.value.weight False\n",
            "bert.encoder.layer.5.attention.self.value.bias False\n",
            "bert.encoder.layer.5.attention.output.dense.weight False\n",
            "bert.encoder.layer.5.attention.output.dense.bias False\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "bert.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "bert.encoder.layer.5.intermediate.dense.weight False\n",
            "bert.encoder.layer.5.intermediate.dense.bias False\n",
            "bert.encoder.layer.5.output.dense.weight False\n",
            "bert.encoder.layer.5.output.dense.bias False\n",
            "bert.encoder.layer.5.output.LayerNorm.weight False\n",
            "bert.encoder.layer.5.output.LayerNorm.bias False\n",
            "bert.encoder.layer.6.attention.self.query.weight True\n",
            "bert.encoder.layer.6.attention.self.query.bias True\n",
            "bert.encoder.layer.6.attention.self.key.weight True\n",
            "bert.encoder.layer.6.attention.self.key.bias True\n",
            "bert.encoder.layer.6.attention.self.value.weight True\n",
            "bert.encoder.layer.6.attention.self.value.bias True\n",
            "bert.encoder.layer.6.attention.output.dense.weight True\n",
            "bert.encoder.layer.6.attention.output.dense.bias True\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.6.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.6.intermediate.dense.weight True\n",
            "bert.encoder.layer.6.intermediate.dense.bias True\n",
            "bert.encoder.layer.6.output.dense.weight True\n",
            "bert.encoder.layer.6.output.dense.bias True\n",
            "bert.encoder.layer.6.output.LayerNorm.weight True\n",
            "bert.encoder.layer.6.output.LayerNorm.bias True\n",
            "bert.encoder.layer.7.attention.self.query.weight True\n",
            "bert.encoder.layer.7.attention.self.query.bias True\n",
            "bert.encoder.layer.7.attention.self.key.weight True\n",
            "bert.encoder.layer.7.attention.self.key.bias True\n",
            "bert.encoder.layer.7.attention.self.value.weight True\n",
            "bert.encoder.layer.7.attention.self.value.bias True\n",
            "bert.encoder.layer.7.attention.output.dense.weight True\n",
            "bert.encoder.layer.7.attention.output.dense.bias True\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.7.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.7.intermediate.dense.weight True\n",
            "bert.encoder.layer.7.intermediate.dense.bias True\n",
            "bert.encoder.layer.7.output.dense.weight True\n",
            "bert.encoder.layer.7.output.dense.bias True\n",
            "bert.encoder.layer.7.output.LayerNorm.weight True\n",
            "bert.encoder.layer.7.output.LayerNorm.bias True\n",
            "bert.encoder.layer.8.attention.self.query.weight True\n",
            "bert.encoder.layer.8.attention.self.query.bias True\n",
            "bert.encoder.layer.8.attention.self.key.weight True\n",
            "bert.encoder.layer.8.attention.self.key.bias True\n",
            "bert.encoder.layer.8.attention.self.value.weight True\n",
            "bert.encoder.layer.8.attention.self.value.bias True\n",
            "bert.encoder.layer.8.attention.output.dense.weight True\n",
            "bert.encoder.layer.8.attention.output.dense.bias True\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.8.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.8.intermediate.dense.weight True\n",
            "bert.encoder.layer.8.intermediate.dense.bias True\n",
            "bert.encoder.layer.8.output.dense.weight True\n",
            "bert.encoder.layer.8.output.dense.bias True\n",
            "bert.encoder.layer.8.output.LayerNorm.weight True\n",
            "bert.encoder.layer.8.output.LayerNorm.bias True\n",
            "bert.encoder.layer.9.attention.self.query.weight True\n",
            "bert.encoder.layer.9.attention.self.query.bias True\n",
            "bert.encoder.layer.9.attention.self.key.weight True\n",
            "bert.encoder.layer.9.attention.self.key.bias True\n",
            "bert.encoder.layer.9.attention.self.value.weight True\n",
            "bert.encoder.layer.9.attention.self.value.bias True\n",
            "bert.encoder.layer.9.attention.output.dense.weight True\n",
            "bert.encoder.layer.9.attention.output.dense.bias True\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.9.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.9.intermediate.dense.weight True\n",
            "bert.encoder.layer.9.intermediate.dense.bias True\n",
            "bert.encoder.layer.9.output.dense.weight True\n",
            "bert.encoder.layer.9.output.dense.bias True\n",
            "bert.encoder.layer.9.output.LayerNorm.weight True\n",
            "bert.encoder.layer.9.output.LayerNorm.bias True\n",
            "bert.encoder.layer.10.attention.self.query.weight True\n",
            "bert.encoder.layer.10.attention.self.query.bias True\n",
            "bert.encoder.layer.10.attention.self.key.weight True\n",
            "bert.encoder.layer.10.attention.self.key.bias True\n",
            "bert.encoder.layer.10.attention.self.value.weight True\n",
            "bert.encoder.layer.10.attention.self.value.bias True\n",
            "bert.encoder.layer.10.attention.output.dense.weight True\n",
            "bert.encoder.layer.10.attention.output.dense.bias True\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.10.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.10.intermediate.dense.weight True\n",
            "bert.encoder.layer.10.intermediate.dense.bias True\n",
            "bert.encoder.layer.10.output.dense.weight True\n",
            "bert.encoder.layer.10.output.dense.bias True\n",
            "bert.encoder.layer.10.output.LayerNorm.weight True\n",
            "bert.encoder.layer.10.output.LayerNorm.bias True\n",
            "bert.encoder.layer.11.attention.self.query.weight True\n",
            "bert.encoder.layer.11.attention.self.query.bias True\n",
            "bert.encoder.layer.11.attention.self.key.weight True\n",
            "bert.encoder.layer.11.attention.self.key.bias True\n",
            "bert.encoder.layer.11.attention.self.value.weight True\n",
            "bert.encoder.layer.11.attention.self.value.bias True\n",
            "bert.encoder.layer.11.attention.output.dense.weight True\n",
            "bert.encoder.layer.11.attention.output.dense.bias True\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.weight True\n",
            "bert.encoder.layer.11.attention.output.LayerNorm.bias True\n",
            "bert.encoder.layer.11.intermediate.dense.weight True\n",
            "bert.encoder.layer.11.intermediate.dense.bias True\n",
            "bert.encoder.layer.11.output.dense.weight True\n",
            "bert.encoder.layer.11.output.dense.bias True\n",
            "bert.encoder.layer.11.output.LayerNorm.weight True\n",
            "bert.encoder.layer.11.output.LayerNorm.bias True\n",
            "bert.pooler.dense.weight True\n",
            "bert.pooler.dense.bias True\n",
            "classifier.weight True\n",
            "classifier.bias True\n"
          ]
        }
      ],
      "source": [
        "# verify fine tuned parameters\n",
        "for name, param in model.named_parameters():\n",
        "    print(name, param.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "H9BZ9IkPs9XV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cff3a88d-effb-4537-ce46-cca5ec210e29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable Parameters: 66959622\n"
          ]
        }
      ],
      "source": [
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"Trainable Parameters: {trainable_params}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}