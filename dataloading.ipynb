{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1462296,"sourceType":"datasetVersion","datasetId":857191}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:11:07.508354Z","iopub.execute_input":"2025-01-26T11:11:07.508870Z","iopub.status.idle":"2025-01-26T11:11:07.516613Z","shell.execute_reply.started":"2025-01-26T11:11:07.508833Z","shell.execute_reply":"2025-01-26T11:11:07.514990Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"pip install pycoco","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T10:34:01.100819Z","iopub.execute_input":"2025-01-26T10:34:01.101187Z","iopub.status.idle":"2025-01-26T10:34:05.509079Z","shell.execute_reply.started":"2025-01-26T10:34:01.101158Z","shell.execute_reply":"2025-01-26T10:34:05.507626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torchvision.datasets as dset\nimport torchvision.transforms as transforms\ncap = dset.CocoCaptions(root = '/kaggle/input/coco-2017-dataset/coco2017/train2017',\n                        annFile = '/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json',\n                        transform=transforms.ToTensor())\n\nprint('Number of samples: ', len(cap))\nimg, target = cap[3] # load 4th sample\n\nprint(\"Image Size: \", img.size())\nprint(target)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T10:34:05.510922Z","iopub.execute_input":"2025-01-26T10:34:05.511330Z","iopub.status.idle":"2025-01-26T10:34:09.113922Z","shell.execute_reply.started":"2025-01-26T10:34:05.511301Z","shell.execute_reply":"2025-01-26T10:34:09.112500Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T10:34:11.435591Z","iopub.execute_input":"2025-01-26T10:34:11.435966Z","iopub.status.idle":"2025-01-26T10:34:15.984703Z","shell.execute_reply.started":"2025-01-26T10:34:11.435939Z","shell.execute_reply":"2025-01-26T10:34:15.983419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForMaskedLM","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:11:15.103259Z","iopub.execute_input":"2025-01-26T11:11:15.103725Z","iopub.status.idle":"2025-01-26T11:11:15.109165Z","shell.execute_reply.started":"2025-01-26T11:11:15.103693Z","shell.execute_reply":"2025-01-26T11:11:15.107946Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"# Load model directly\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\n\ntokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\nmodel = AutoModelForMaskedLM.from_pretrained(\"google-bert/bert-base-uncased\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:11:16.411709Z","iopub.execute_input":"2025-01-26T11:11:16.412074Z","iopub.status.idle":"2025-01-26T11:11:17.191955Z","shell.execute_reply.started":"2025-01-26T11:11:16.412045Z","shell.execute_reply":"2025-01-26T11:11:17.190783Z"}},"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at google-bert/bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"\n\nimport os\nimport json\nfrom PIL import Image\nfrom pycocotools.coco import COCO\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport random\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:11:19.998517Z","iopub.execute_input":"2025-01-26T11:11:19.998911Z","iopub.status.idle":"2025-01-26T11:11:20.004252Z","shell.execute_reply.started":"2025-01-26T11:11:19.998881Z","shell.execute_reply":"2025-01-26T11:11:20.002852Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"\nclass COCOImageCaptionDataset(Dataset):\n    def __init__(self, img_dir, annotations_file, transform=None):\n        self.img_dir = img_dir\n        \n        # Load annotations\n        with open(annotations_file, 'r') as f:\n            self.annotations = json.load(f)\n        \n        # Define default transform if not provided\n        self.transform = transform or transforms.Compose([\n            transforms.RandomGrayscale(p=0.3),\n            transforms.RandomRotation(5),\n            transforms.RandomHorizontalFlip(p=0.5),\n            transforms.ColorJitter(brightness=0.1, contrast=0.1),\n            transforms.RandomResizedCrop(224, scale=(0.9, 1.0)),\n            transforms.GaussianBlur(kernel_size=(3, 3), sigma=(0.1, 1.0)),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n        ])\n        \n        # Initialize tokenizer\n        self.tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n\n        \n        # Get list of image files\n        self.image_files = [\n            os.path.join(img_dir, file)\n            for file in os.listdir(img_dir)\n            if file.endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))\n        ]\n    \n    def __len__(self):\n        return len(self.annotations['annotations'])\n    \n    def __getitem__(self, idx):\n        # Get annotation\n        ann = self.annotations['annotations'][idx]\n        \n        # Load image\n        img_path = f\"{self.img_dir}/{ann['image_id']:012d}.jpg\"\n        image = Image.open(img_path).convert('RGB')\n        \n        # Apply transformations\n        if self.transform:\n            image = self.transform(image)\n        \n        # Anchor caption\n        print(ann)\n        caption = ann['caption']\n        encoding = self.tokenizer(caption, padding='max_length', \n                                  truncation=True, max_length=64,\n                                  return_tensors='pt')\n        \n        # Negative caption (random from dataset)\n        neg_idx = random.choice([i for i in range(len(self)) if i != idx])\n        \n        neg_caption = self.annotations['annotations'][neg_idx]['caption']\n        print(neg_caption)\n        neg_encoding = self.tokenizer(neg_caption, padding='max_length',\n                                      truncation=True, max_length=64,\n                                      return_tensors='pt')\n        \n        return {\n            'image': image,\n            'caption_ids': encoding['input_ids'].squeeze(0),\n            'caption_mask': encoding['attention_mask'].squeeze(0),\n            'neg_caption_ids': neg_encoding['input_ids'].squeeze(0),\n            'neg_caption_mask': neg_encoding['attention_mask'].squeeze(0)\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:11:21.755786Z","iopub.execute_input":"2025-01-26T11:11:21.756162Z","iopub.status.idle":"2025-01-26T11:11:21.768171Z","shell.execute_reply.started":"2025-01-26T11:11:21.756133Z","shell.execute_reply":"2025-01-26T11:11:21.766722Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"ob = COCOImageCaptionDataset(\"/kaggle/input/coco-2017-dataset/coco2017/train2017\", \"/kaggle/input/coco-2017-dataset/coco2017/annotations/captions_train2017.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:11:24.814991Z","iopub.execute_input":"2025-01-26T11:11:24.815378Z","iopub.status.idle":"2025-01-26T11:11:26.870549Z","shell.execute_reply.started":"2025-01-26T11:11:24.815347Z","shell.execute_reply":"2025-01-26T11:11:26.869538Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:13:00.416092Z","iopub.execute_input":"2025-01-26T11:13:00.416480Z","iopub.status.idle":"2025-01-26T11:13:00.506808Z","shell.execute_reply.started":"2025-01-26T11:13:00.416453Z","shell.execute_reply":"2025-01-26T11:13:00.505652Z"}},"outputs":[{"name":"stdout","text":"{'image_id': 106140, 'id': 98, 'caption': 'A large passenger airplane flying through the air.'}\nA large pink and white bird standing in a pool of water.\n","output_type":"stream"},{"execution_count":58,"output_type":"execute_result","data":{"text/plain":"{'image': tensor([[[-1.4158, -0.8678, -0.4739,  ..., -2.1179, -2.1179, -2.1179],\n          [-1.2959, -0.7822, -0.4397,  ..., -2.1179, -2.1179, -2.1179],\n          [-1.1760, -0.6965, -0.4054,  ..., -2.1179, -2.1179, -2.1179],\n          ...,\n          [-2.1179, -2.1179, -2.1179,  ..., -1.1589, -1.1760, -1.1589],\n          [-2.1179, -2.1179, -2.1179,  ..., -1.1589, -1.1075, -1.0390],\n          [-2.1179, -2.1179, -2.1179,  ..., -1.1075, -1.0048, -0.8507]],\n \n         [[-1.3004, -0.7227, -0.2850,  ..., -2.0357, -2.0357, -2.0357],\n          [-1.1779, -0.6352, -0.2675,  ..., -2.0357, -2.0357, -2.0357],\n          [-1.0203, -0.5126, -0.2325,  ..., -2.0357, -2.0357, -2.0357],\n          ...,\n          [-2.0357, -2.0357, -2.0357,  ..., -0.7927, -0.7752, -0.7752],\n          [-2.0357, -2.0357, -2.0357,  ..., -0.7577, -0.7227, -0.6877],\n          [-2.0357, -2.0357, -2.0357,  ..., -0.7227, -0.6877, -0.6352]],\n \n         [[-1.0027, -0.3753,  0.0779,  ..., -1.8044, -1.8044, -1.8044],\n          [-0.8633, -0.2881,  0.1128,  ..., -1.8044, -1.8044, -1.8044],\n          [-0.7064, -0.1487,  0.1651,  ..., -1.8044, -1.8044, -1.8044],\n          ...,\n          [-1.8044, -1.8044, -1.8044,  ..., -0.2184, -0.2532, -0.2707],\n          [-1.8044, -1.8044, -1.8044,  ..., -0.2010, -0.2358, -0.2532],\n          [-1.8044, -1.8044, -1.8044,  ..., -0.2184, -0.2532, -0.2707]]]),\n 'caption_ids': tensor([  101,  1037,  2312,  4628, 13297,  3909,  2083,  1996,  2250,  1012,\n           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0]),\n 'caption_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 'neg_caption_ids': tensor([ 101, 1037, 2312, 5061, 1998, 2317, 4743, 3061, 1999, 1037, 4770, 1997,\n         2300, 1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n            0,    0,    0,    0]),\n 'neg_caption_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])}"},"metadata":{}}],"execution_count":58},{"cell_type":"code","source":"from torch import nn","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:22:05.216885Z","iopub.execute_input":"2025-01-26T11:22:05.217269Z","iopub.status.idle":"2025-01-26T11:22:05.222196Z","shell.execute_reply.started":"2025-01-26T11:22:05.217237Z","shell.execute_reply":"2025-01-26T11:22:05.220763Z"}},"outputs":[],"execution_count":61},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n    def __init__(self, projection_dim=128):\n        super(ImgEncoder_CNN, self).__init__()\n        \n        base_model = models.resnet50(pretrained=False)\n        self.base_model = nn.Sequential(*list(base_model.children())[:-1])\n        \n        # Freezeing the parameters of the base model\n        for param in self.base_model.parameters():\n            param.requires_grad = False  # Corrected attribute name\n        \n        # Define the projection head\n        self.projection_head = ProjectionHead(2048, 256, projection_dim)  # Corrected input_dim\n\n    def forward(self, x):\n    \n        # Extract features from the base model\n        h = self.base_model(x).squeeze()  # Shape: [batch_size, 2048]\n        \n        # Pass through the projection head\n        z = self.projection_head(h)  # Shape: [batch_size, projection_dim]\n        \n        # Normalize the output embeddings\n        return normalize(z, dim=1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-26T11:22:06.689886Z","iopub.execute_input":"2025-01-26T11:22:06.690383Z","iopub.status.idle":"2025-01-26T11:22:06.698415Z","shell.execute_reply.started":"2025-01-26T11:22:06.690346Z","shell.execute_reply":"2025-01-26T11:22:06.696549Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}