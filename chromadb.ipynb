{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import chromadb\nimport matplotlib.pyplot as plt\nchroma_client = chromadb.PersistentClient(path=\"./chroma_db\") \ncollection = chroma_client.get_or_create_collection(\"image_collection\")\n\n# Function to get image paths\ndef preprocess_image(image_path):\n    transform = transforms.Compose([\n        transforms.Resize((224, 224)), \n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n    ])\n    image = Image.open(image_path).convert(\"RGB\")\n    return transform(image).unsqueeze(0)\n\ndef get_image_paths(directory):\n    valid_extensions = (\".jpg\", \".jpeg\", \".png\", \".bmp\", \".webp\")\n    return [os.path.join(directory, f) for f in os.listdir(directory) if f.lower().endswith(valid_extensions)]\n    \ndef image_embeddings(model, image_paths):\n    model.eval()\n    \n    image_tensors = [preprocess_image(img_path).to(device) for img_path in image_paths]\n    image_tensors = torch.cat(image_tensors, dim=0)\n\n    with torch.no_grad():\n        image_features = model.image_encoder(image_tensors)\n        image_features = F.normalize(image_features, dim=-1)\n    return image_features.tolist()\n\n# Get image paths\nimage_dir = \"/kaggle/input/ritesh-gallery\"\nimage_paths = get_image_paths(image_dir)\nembeddings=image_embeddings(model,image_paths)\n\n# Process and store in ChromaDB\ndata = [\n    {\"id\": str(idx), \"embedding\": embedding, \"metadata\": {\"path\": img_path}}\n    for idx, (embedding, img_path) in enumerate(zip(embeddings, image_paths))\n]\n\n\ncollection.add(\n    ids=[item[\"id\"] for item in data],  \n    embeddings=[item[\"embedding\"] for item in data],  \n    metadatas=[item[\"metadata\"] for item in data]\n)\n\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"query=\"laptop screen\"\n\nwith torch.no_grad():\n        query_features = model.text_encoder([query]).to(device)\n        query_features = F.normalize(query_features, dim=-1)\n\nquery_features=query_features.tolist()\n\n\nresults = collection.query(\n    query_embeddings=query_features,  \n    n_results=1  \n)\n\nbest_match_path = results[\"metadatas\"][0][0][\"path\"]\nprint(\"Best matching image:\", best_match_path)\n\n\nimage = Image.open(best_match_path)\nplt.figure(figsize=(6, 6))\nplt.imshow(image)\nplt.axis(\"off\")  \nplt.title(f\"Query: {query}\", fontsize=14, fontweight=\"bold\", color=\"blue\")  \nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}