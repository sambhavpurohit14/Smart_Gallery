{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPqs8PDlQnWM9aokj16zPo7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sambhavpurohit14/Smart_Gallery/blob/encoder-functions/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_kaw_YFqYmOf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "sentence -> tokens -> inputs ID (location in the vocab) -> embedding\n",
        "'''\n",
        "\n",
        "import torch\n",
        "import torch.nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceEmbeddings(nn.Module):\n",
        "\n",
        "  def __init__(self, model_dimensions : int, vocab_size : int):\n",
        "    super().__init__()\n",
        "    self.model_dimensions = model_dimensions\n",
        "    self.vocab_size = vocab_size\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.model_dimensions)\n",
        "\n",
        "    '''\n",
        "    embedding maps the numbers to a vector, and those numbers are learned during training\n",
        "    paper multiplies the embeddings by sqrt(model_dimensions)\n",
        "    '''\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    return self.embedding(inputs) * math.sqrt(self.model_dimensions)"
      ],
      "metadata": {
        "id": "UyltqALAY6Y-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "\n",
        "  def __init__(self, model_dimensions : int, max_length : int, dropout : float) -> None:\n",
        "    super().__init__()\n",
        "    self.model_dimensions = model_dimensions\n",
        "    self.max_length = max_length\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    '''\n",
        "    we need vectors of d model size and max_length number\n",
        "    even positions - sines, odd positions cos\n",
        "    '''\n",
        "    # numerical stability\n",
        "    position_encoding = torch.zeros(max_length, model_dimensions)\n",
        "    position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "    denominator = torch.exp(torch.arange(0, model_dimensions, 2).float() * (-math.log(10000.0) / model_dimensions))\n",
        "\n",
        "    position_encoding[:,0::2] = torch.sin(position * demominator)\n",
        "    position_encoding[:,1::2] = torch.cos(position * denominator)\n",
        "    '''\n",
        "    keep in model, but not as a parameter - buffer\n",
        "    '''\n",
        "    positional_encoding = position_encoding.unsqueeze(0).transpose(0,1)\n",
        "    self.register_buffer('positional_encoding', positional_encoding)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    inputs = inputs + (self.positional_encoding[:, :x.shape[1], :]).requires_grad_(False)\n",
        "    return self.dropout(inputs)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0cWMneSSctkB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "layer normalization - calculate mean and variance for each item independently of others\n",
        "\n",
        "'''\n",
        "class LayerNormalization(nn.Module):\n",
        "\n",
        "  def __init__(self, epsilon: float = 10** -8) -> None:\n",
        "    super().__init__()\n",
        "    self.epsilon = epsilon\n",
        "    self.w = nn.Parameter(torch.ones(1))\n",
        "    self.b = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    mean = inputs.mean(dim = -1, keepdim = True)\n",
        "    std = x.std(dim = -1, keepdim = True)\n",
        "    return self.alpha * (x - mean) / (std + self.epsilon) + self.b"
      ],
      "metadata": {
        "id": "1eeN9whEy3sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "\n",
        "  def __init__(self, model_dimensions : int, ff : int, dropout:float) -> None:\n",
        "    super().__init__()\n",
        "    self.linear_1 = nn.Linear(model_dimensions, ff)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.linear_2 = nn.Linear(ff, model_dimensions)\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    # change of dimensions\n",
        "    return self.linear_2(self.dropout(torch.relu(self.linear_1(inputs))))"
      ],
      "metadata": {
        "id": "f6y2rePg2uo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' multihead attention\n",
        "define 3 matrices q, k, v\n",
        "in case of encoder, all are same\n",
        "W is learned\n",
        "\n",
        "attention = softmax((QK^T) / sqrt(d_k)\n",
        "get 1D heads, multiply by Wo\n",
        " '''\n",
        "\n",
        " class MultiHeadAttention(nn.Module):\n",
        "\n",
        "  def __init__(self, model_dimensions : int, h:int, dropout : float) -> None:\n",
        "    super().__init__()\n",
        "    self.model_dimensions = model_dimensions\n",
        "    self.h = h\n",
        "    assert model_dimensions % h == 0, \"CHECK Y DIS\"\n",
        "    self.d_k = model_dimensions // h\n",
        "\n",
        "    self.w_q = nn.Linear(model_dimensions, model_dimensions)\n",
        "    self.w_k = nn.Linear(model_dimensions, model_dimensions)\n",
        "    self.w_v = nn.Linear(model_dimensions, model_dimensions)\n",
        "\n",
        "    self.w_o = nn.Linear(model_dimensions, model_dimensions)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  @staticmethod\n",
        "  def attention(query, key, value, mask, dropout:nn.Dropout):\n",
        "    d_k = query.shape[-1]\n",
        "    attention_scores = (query @ key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "      attention_scores.masked_fill_(mask == 0, -1e9)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, q, k, v, mask):\n",
        "    '''\n",
        "    check dimensions with paper, should not change\n",
        "        '''\n",
        "    query = self.w_q(q)\n",
        "    key = self.w_k(k)\n",
        "    value = self.w_v(v)\n",
        "    ''''\n",
        "    batch,max_len,model_dim -> batch,max_len,h,d_k\n",
        "    '''\n",
        "    query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    key = query.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "    value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2)\n",
        "\n",
        "    x, self.attention_scores = MultiHeadAttention.attention(query, key, value, mask, self.dropout)\n",
        "    # exchange 1 and 2, more swap than transpose\n",
        "    x = x.transpose(1, 2).contiguos().view(x.shape[0], -1, self.h * self.d_k)\n",
        "    return self.w_o(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "rsHBwXH63qSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "\n",
        "  def __init__(self, dropout : float) -> None:\n",
        "    super().__init__()\n",
        "    self.layer_norm = LayerNormalization()\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, inputs, sublayer):\n",
        "    return x + self.dropout(sublayer(self.layer_norm(inputs)))"
      ],
      "metadata": {
        "id": "chSX4Bd8cUOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "\n",
        "  def __init__(self, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForward, dropout: float) -> None:\n",
        "    super().__init__()\n",
        "    self.self_attention_block = self_attention_block\n",
        "    self.feed_forward_block = feed_forward_block\n",
        "    self.residual_connections = n.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "  def forward(self, inputs, mask):\n",
        "    x = self.residual_connections[0](inputs, lambda x: self.self_attention_block(x, x, x, mask))\n",
        "    x = self.residual_connections[1](x, self.feed_forward_bock)\n",
        "    return x"
      ],
      "metadata": {
        "id": "2Qjgj5UKdgIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "  def __init__(self, layers: n.ModuleList) -> None:\n",
        "    super().__init__()\n",
        "    self.layers = layers\n",
        "    self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "    x = inputs\n",
        "    for layer in self.layers:\n",
        "      x = layer(x, mask)\n",
        "    return self.norm(x)"
      ],
      "metadata": {
        "id": "fM0-Jx1TeVCt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}