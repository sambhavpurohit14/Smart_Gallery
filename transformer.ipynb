{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeR9V1QzPaxlMF1gEdU7pE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sambhavpurohit14/Smart_Gallery/blob/encoder-functions/transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_kaw_YFqYmOf"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "sentence -> tokens -> inputs ID (location in the vocab) -> embedding\n",
        "'''\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import math"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "    embedding maps the numbers to a vector, and those numbers are learned during training\n",
        "     in paper multiply the embeddings by sqrt(model_dimensions)\n",
        "'''\n",
        "class SentenceEmbeddings(nn.Module):\n",
        "    def __init__(self, model_dimensions: int, vocab_size: int):\n",
        "        super().__init__()\n",
        "        self.model_dimensions = model_dimensions\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(self.vocab_size, self.model_dimensions)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.embedding(inputs) * math.sqrt(self.model_dimensions)\n"
      ],
      "metadata": {
        "id": "UyltqALAY6Y-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, model_dimensions: int, max_length: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.model_dimensions = model_dimensions\n",
        "        self.max_length = max_length\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        position_encoding = torch.zeros(max_length, model_dimensions)\n",
        "        position = torch.arange(0, max_length, dtype=torch.float).unsqueeze(1)\n",
        "        denominator = torch.exp(torch.arange(0, model_dimensions, 2).float() * (-math.log(10000.0) / model_dimensions))\n",
        "\n",
        "        position_encoding[:, 0::2] = torch.sin(position * denominator)\n",
        "        position_encoding[:, 1::2] = torch.cos(position * denominator)\n",
        "\n",
        "        position_encoding = position_encoding.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('positional_encoding', position_encoding)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs = inputs + self.positional_encoding[:inputs.size(0), :]\n",
        "        return self.dropout(inputs)\n"
      ],
      "metadata": {
        "id": "0cWMneSSctkB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "layer normalization - calculate mean and variance for each item independently of others\n",
        "\n",
        "'''\n",
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, epsilon: float = 1e-8):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        self.w = nn.Parameter(torch.ones(1))\n",
        "        self.b = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        mean = inputs.mean(dim=-1, keepdim=True)\n",
        "        std = inputs.std(dim=-1, keepdim=True)\n",
        "        return self.w * (inputs - mean) / (std + self.epsilon) + self.b"
      ],
      "metadata": {
        "id": "1eeN9whEy3sc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, model_dimensions: int, ff: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.linear_1 = nn.Linear(model_dimensions, ff)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear_2 = nn.Linear(ff, model_dimensions)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.linear_2(self.dropout(torch.relu(self.linear_1(inputs))))"
      ],
      "metadata": {
        "id": "f6y2rePg2uo1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "''' multihead attention\n",
        "define 3 matrices q, k, v\n",
        "in case of encoder, all are same\n",
        "W is learned\n",
        "\n",
        "attention = softmax((QK^T) / sqrt(d_k)\n",
        "get 1D heads, multiply by Wo\n",
        " '''\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, model_dimensions: int, h: int, dropout: float):\n",
        "        super().__init__()\n",
        "        self.model_dimensions = model_dimensions\n",
        "        self.h = h\n",
        "        assert model_dimensions % h == 0, \"Model dimensions must be divisible by the number of heads.\"\n",
        "        self.d_k = model_dimensions // h\n",
        "\n",
        "        self.w_q = nn.Linear(model_dimensions, model_dimensions)\n",
        "        self.w_k = nn.Linear(model_dimensions, model_dimensions)\n",
        "        self.w_v = nn.Linear(model_dimensions, model_dimensions)\n",
        "        self.w_o = nn.Linear(model_dimensions, model_dimensions)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def attention(self, query, key, value, mask=None):\n",
        "        d_k = query.size(-1)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "        attention_weights = self.dropout(attention_weights)\n",
        "        return torch.matmul(attention_weights, value), attention_weights\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        batch_size = q.size(0)\n",
        "\n",
        "        query = self.w_q(q).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        key = self.w_k(k).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "        value = self.w_v(v).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
        "\n",
        "        x, _ = self.attention(query, key, value, mask)\n",
        "\n",
        "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.model_dimensions)\n",
        "        return self.w_o(x)"
      ],
      "metadata": {
        "id": "rsHBwXH63qSf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    def __init__(self, dropout: float):\n",
        "        super().__init__()\n",
        "        self.layer_norm = LayerNormalization()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs, sublayer):\n",
        "        return inputs + self.dropout(sublayer(self.layer_norm(inputs)))\n"
      ],
      "metadata": {
        "id": "chSX4Bd8cUOQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, self_attention_block: MultiHeadAttention, feed_forward_block: FeedForward, dropout: float):\n",
        "        super().__init__()\n",
        "        self.self_attention_block = self_attention_block\n",
        "        self.feed_forward_block = feed_forward_block\n",
        "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)])\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "        x = self.residual_connections[0](inputs, lambda x: self.self_attention_block(x, x, x, mask))\n",
        "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
        "        return x"
      ],
      "metadata": {
        "id": "2Qjgj5UKdgIF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layers: nn.ModuleList):\n",
        "        super().__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization()\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "        x = inputs\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ],
      "metadata": {
        "id": "fM0-Jx1TeVCt"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"The quick brown fox jumps over the lazy dog\""
      ],
      "metadata": {
        "id": "I2jTziQrjiF-"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}