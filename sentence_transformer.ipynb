{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNw6VkwMTJVuD0HxcXyddz5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sambhavpurohit14/Smart_Gallery/blob/encoder-functions/sentence_transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "5yJM8O7DVHKZ"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, model_name: str, out_dim: int = 512):\n",
        "        super(TextEncoder, self).__init__()\n",
        "\n",
        "        # pre-trained sentence transformer\n",
        "        self.sentence_transformer = SentenceTransformer(model_name)\n",
        "\n",
        "        # freeze the Sentence Transformer layers, only the projection head is trainable\n",
        "        for param in self.sentence_transformer.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        #  projection layer\n",
        "        self.projection = nn.Sequential(\n",
        "            nn.Linear(self.sentence_transformer.get_sentence_embedding_dimension(), out_dim),\n",
        "            nn.LayerNorm(out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, sentences):\n",
        "        # encode\n",
        "        with torch.no_grad():  # frozen weightts\n",
        "            sentence_embeddings = self.sentence_transformer.encode(sentences, convert_to_tensor=True)\n",
        "\n",
        "        # project 768 to 512\n",
        "        projected_embeddings = self.projection(sentence_embeddings)\n",
        "        return projected_embeddings\n"
      ],
      "metadata": {
        "id": "4fJ05C3VuvOq"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    model_name = \"all-mpnet-base-v2\"\n",
        "    out_dim = 512\n",
        "\n",
        "    # Initialize the encoder\n",
        "    text_encoder = TextEncoder(model_name=model_name, out_dim=out_dim)\n",
        "    sentences = [\"acm sanganitra is the best\"]\n",
        "    embeddings = text_encoder(sentences)\n",
        "\n",
        "    print(embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGGdsDRBu2Z2",
        "outputId": "9c4b5da9-f2b6-4ebc-9c40-a41d42d981bd"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1BF59eou-WU",
        "outputId": "d91d17ce-a894-4464-ef8b-1ca0434f6108"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchinfo in /usr/local/lib/python3.11/dist-packages (1.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "print(text_encoder)\n",
        "\n",
        "# Count trainable parameters\n",
        "def count_trainable_params(model):\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    frozen_params = total_params - trainable_params\n",
        "    return total_params, trainable_params, frozen_params\n",
        "\n",
        "total_params, trainable_params, frozen_params = count_trainable_params(text_encoder)\n",
        "\n",
        "print(\"\\nParameter Summary:\")\n",
        "print(f\"Total Parameters: {total_params}\")\n",
        "print(f\"Trainable Parameters: {trainable_params}\")\n",
        "print(f\"Frozen Parameters: {frozen_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAcWfD3ByTyK",
        "outputId": "951bbd80-a55c-482c-b131-03623899b73e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TextEncoder(\n",
            "  (sentence_transformer): SentenceTransformer(\n",
            "    (0): Transformer({'max_seq_length': 384, 'do_lower_case': False}) with Transformer model: MPNetModel \n",
            "    (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "    (2): Normalize()\n",
            "  )\n",
            "  (projection): Sequential(\n",
            "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
            "    (1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Parameter Summary:\n",
            "Total Parameters: 109881216\n",
            "Trainable Parameters: 394752\n",
            "Frozen Parameters: 109486464\n"
          ]
        }
      ]
    }
  ]
}